<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on 打码匠</title>
    <link>http://blog.ibyte.me/post/</link>
    <description>Recent content in Posts on 打码匠</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 10 Nov 2021 17:22:55 +0800</lastBuildDate>
    
	<atom:link href="http://blog.ibyte.me/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>How Redis Solves the CAP Problem</title>
      <link>http://blog.ibyte.me/post/how-redis-solves-the-cap-problem/</link>
      <pubDate>Wed, 10 Nov 2021 17:22:55 +0800</pubDate>
      
      <guid>http://blog.ibyte.me/post/how-redis-solves-the-cap-problem/</guid>
      <description>CAP理论  CAP理论是什么？ 可以这么说吧cap理论是分布式系统的奠基石吧，这个理论论述分布式系统设计的3个最大问题：
 一致性 (Consistency) 可用性 (Availability) 分区容错性 (Partition Tolerance)  什么是一致性？假设一个集群有2个节点A-1和A-2，这两个节点为主从关系，例如redis里面的主从模式。主节点负责写入数据，从节点负责从主节点同步数据到本机。
A-1和A-2 通过网络进行数据同步，要怎么保证两台机子上的数据一致的呢？这个就是数据一致性问题。
那什么是可用性？A-1和A-2两台机子做成一个集群，实际上真实情况可能更多，在集群中一部分节点故障后，集群整体是否还能响应客户端的读写请求，例如A-1挂掉以后，A-2会马上接管这些请求的，保持整体可用性。
那什么是分区容错性？一个集群都靠着网络进行通讯的，如果网络波动，这个集群中的部分节点网络和集群上的其他节点联通不了，那么数据一致性就很难保持了，网络一断开，那么那么整个集群可能被划分成多个独立的小分区了，如果在短时间内不能联通同步数据，这就是出现网络分区问题。
如上图 A-1和A-3网络连接出现了故障，那么这个集群就已经存在了网络分区问题！但是整体集群而看上去却好像是在一个可以运转正常，其他剩下的机器还能够正常运转满足系统需求，对于用户而言并没有什么体验上的影响。
所以在设计分布式系统时，CAP问题必会出现！除非你玩单机系统😜！怎么优雅解决问题，就看设计者了在A和C、P之前怎么做决策了。用这些问题我去套用在redis集群模式上，看redis集群是怎么解决CAP问题的。
   Redis哨兵  Redis早期版本，redis单机情况下即便性能强的一批，但是大规模数据或者海量请求下还是为支撑不了可用性和稳定性，后面redis官方有一个解决方案就是哨兵模式。
哨兵模式：上面提到了主从架构，主从切换技术方案是，当主服务器宕机后，需要手动把一台从服务器切换为主服务器，这中间就需要人工干预，例如大半夜收到报警，爬起来手动切换服务器？？？需要手动将 从节点 晋升为 主节点，同时还要通知 客户端 更新 主节点地址，显然这种问题怎么可能忍？大家都是程序员自己写个程序自动化不就好了，为什么要用这么笨方式。。。哨兵模式哨兵就是解决这个问题的，哨兵是一个独立的进程，作为进程，它会独立运行，其原理是哨兵通过发送命令，等待Redis服务器响应，从而监控运行的多个Redis实例，当有一个节点挂掉的时候，客户端连接其节点失败，就会向哨兵服务器询问新的节点地址然后继续使用，其他从节点也会跟着把主节点调整到新节点上，哨兵还会记录当前挂掉节点状态，还会定时进行查看如果复活了还会添加到哨兵集群里，从而达到无人干扰。
   增量同步&amp;amp;amp;全量同步  上面哨兵模式切换过程，提到了主从，那主从节点数据，怎么保证一致性的的？在redis集群情况下同步模式有两种：
 增量同步 全量同步  设想一下主从节点在某个网络情况出现问题波动，这是客户端正在朝着服务器端主节点，写数据，而从节点又在从主节点读取同步被写入的数据，这时网络一波动，那么就出现网络分区了。出现网络分区了，此时两个节点就处于独立的单机状态，如图：
正如上图所话的，主从在短时间内出现网络分区，redis的设计者针对这个方案提出了一个方案叫增量复制，如下图，当然这个图我之前讲go中的channel的时候画的，反正是同用的，所以懒得话了，直接拿过来用一用。
redis在每个主节点上分配一快ring也可以叫为buf，这个ring是里面就记录它自身大小元素个数的最近客户端执行命令记录，如果从节点短时间内恢复了链接，此时从节点直接去主节点的ring中拿取命令记录数据，然后在自己本机上跑一遍。
增量复制不是没有问题的，设想一下如果这个网络分区出现很长时间，这个ring容量是不是不够用，所有第二套解决方案出现了，全量同步这个过程是怎么样的？
直接把redis在内存里面的数据通过序列化然后通过网络发送到从节点，从节点在本地保存，然后全量加载到内存里面完成数据恢复，在全量同步的时候，可能也有其他命令漏掉，还会配上增量复制进行。
   小结  目前只说到Redis哨兵模式了，这个模式能解决一部分CAP问题，例如还有：节点动态添加，数据动态迁移。。。这些问题在下一篇文章写，也是经典的分而治之的应用。</description>
    </item>
    
    <item>
      <title>Why Is Redis High Performance</title>
      <link>http://blog.ibyte.me/post/why-is-redis-high-performance/</link>
      <pubDate>Thu, 04 Nov 2021 14:32:49 +0800</pubDate>
      
      <guid>http://blog.ibyte.me/post/why-is-redis-high-performance/</guid>
      <description>Redis的高性能怎么做到的？  Redis这个NOSQL数据库在计算机界可谓是无人不知，无人不晓。只要涉及到数据那么就需要数据库，数据库类型很多，但是NOSQL的kv内存数据库也很多，redis作为其中一个是怎么做到行业天花板的呢？是怎么做到高性能的呢？怎么做到高可用的呢？今天这篇八股文我就整理一些redis的设计写写，本篇还是偏关于高性能这一块。
   高效数据结构  Redis的数据库相比传统的关系数据库，在数据结构上也是比较特殊的，它的所有数据类型都可以看做是一个map的结构，key作为查询条件。
Redis基于KV内存数据库，它内部构建了一个哈希表，根据指定的KEY访问时，只需要O(1)的时间复杂度就可以找到对应的数据，而value的值又是一些拥有各种特性的数据结构，这就给redis在数据操作的时候提供很好的性能了。
   基于内存存储  相比传统的关系数据库，数据文件可能以lsm tree 或者 b+ tree形式存在硬盘上，这个时候读取文件要有io操作了，而redis在内存中进行，并不会大量消耗CPU资源，所以速度极快。
内存从上图可以看到它介于硬盘和cpu缓存中间的，相比硬盘查找数据肯定是快的，当然这里笔者个人见解上，如果关系型数据库把一些平凡操作的数据库也放置在内存中缓存，也会得到一些性能的提升，像操作系统里面缺页异常一样处理，把数据片段通过一些特殊算法缓存在内存里面，减少文件io的开销。
   io多路复用  传统对于并发情况，假如一个进程不行，那搞多个进程不就可以同时处理多个客户端连接了么？多进程是可以解决一些并发问题，但是还是有一些问题，上下文切换开销，线程循环创建，从PCB来回恢复效率较低。随着客户端请求增多，那么线程也随着请求数量直线上升，如果是并发的时候涉及到数据共享访问，有时候涉及到使用锁来控制范围顺序，影响其他线程执行效率。（进程在Linux也可以理解为线程，每个进程只是有一个线程，当然这里我上面写的进程，别纠结这些。。。）
线程是运行在进程上下文的逻辑流，一个进程可以包含多个线程，多个线程运行在同一进程上下文中，因此可共享这个进程地址空间的所有内容，解决了进程与进程之间通信难的问题，同时，由于一个线程的上下文要比一个进程的上下文小得多，所以线程的上下文切换，要比进程的上下文切换效率高得多。
像redis和Nginx这种应用就是单线程的程序，为什么他们能做到这么强的性能？首先看一个例子：
 Blocking IO   中午吃饭，我给餐厅老板说要一碗‘热干面’，然后我就在那边一直等着老板做，老板没有做好，我就一直在哪里等着什么也不做，直到‘热干面’做好。
 这个流程就是我们常说的Blocking I/O如图：
同步阻塞 IO 模型中，应用程序发起 read 调用后，会一直阻塞，直到内核把数据拷贝到用户空间。
 Non Blocking IO  切换一下常见：
 同样你中午吃饭，给餐厅老板说要一碗‘热干面’，然后老板开始做了，你每隔几分钟向老板问一下‘好了吗？’，直到老板说好了，你取到‘热干面’结束。
 同步非阻塞 IO 模型中，应用程序会一直发起read调用，等待数据从内核空间拷贝到用户空间的这段时间里，线程依然是阻塞的，直到在内核把数据拷贝到用户空间，通过轮询操作，避免了一直阻塞，取回热干面的过程就是内核把准备好的数据交换到用户空间过程。
综上两种模型，缺点都是差不多，都是在等待内核准备数据，然后阻塞等待，同样逃不开阻塞这个问题，应用程序不断进行I/O系统调用轮询数据是否已经准备好的过程是十分消耗CPU资源的。
I/O Multiplexing  还是之前那个例子：
 中午吃饭，给餐厅老板说要一碗‘热干面’，然后老板安排给下面的厨子做，具体哪个厨子做不知道，有好几个厨子，然后老板每隔一段时间询问下面的厨子有木有做好，如果做好了，就通知我来去取餐。
 IO 多路复用模型中，线程首先发起 select 调用，询问内核数据是否准备就绪，等内核把数据准备好了，用户线程再发起 read 调用，read 调用的过程（数据从内核空间-&amp;gt;用户空间）还是阻塞的。</description>
    </item>
    
    <item>
      <title>Redis Current Limiting</title>
      <link>http://blog.ibyte.me/post/redis-current-limiting/</link>
      <pubDate>Thu, 28 Oct 2021 21:41:23 +0800</pubDate>
      
      <guid>http://blog.ibyte.me/post/redis-current-limiting/</guid>
      <description>Current Limiting  在编写系统时候有时候我们的系统在设计的时候就已经估算到了最大请求负载了，如果大量的请求超过系统所能承受着的值时，那么系统可能随时挂掉，所有聪明程序员就想到了请求限流来控制系统的可用和稳定性。
   滑动窗口限流  滑动窗口算法将一个大的时间窗口分成多个小窗口，每次大窗口向后滑动一个小窗口，并保证大的窗口内流量不会超出最大值，这种实现比固定窗口的流量曲线更加平滑。
以系统限制用户行为为例子，比如一秒内进行某个操作5次，这种行为应该进行限制，滑动窗口就是记录一个滑动的时间窗口内的操作次数，操作次数超过阈值则进行限流。
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  public boolean isActionAllowed(String userId, String actionKey, int period, int maxCount) { // 生成唯一的key  String key = String.format(&amp;#34;hist:%s:%s&amp;#34;, userId, actionKey); long nowTs = System.currentTimeMillis(); // 使用管道  Pipeline pipe = jedis.pipelined(); pipe.multi(); // 添加当前操作当zset中  pipe.</description>
    </item>
    
    <item>
      <title>Hyperloglog</title>
      <link>http://blog.ibyte.me/post/hyperloglog/</link>
      <pubDate>Mon, 25 Oct 2021 14:18:04 +0800</pubDate>
      
      <guid>http://blog.ibyte.me/post/hyperloglog/</guid>
      <description>Hyperloglog  上一篇文章HLL基础部分介绍了HLL一些相关理论知识，里面的抛硬币游戏，硬币正反两面，如果用计算机里面的数据表示的话，完全可以使用0和1，1表示正面，0表示反面。
HLL常用于去重场景，HLL 算法需要完整遍历所有元素一次，而非多次或采样，算法只能计算集合中有多少个不重复的元素，不能给出每个元素的出现次数或是判断一个元素是否之前出现过，多个使用 HLL 统计出的基数值 可以融合。
HLL的空间只会和精度有关，下面的后面不同的数字代表着不同的精度，数字越大，精度越高，占用的空间也越大。
 上一篇文章HLL基础部分介绍了HLL一些相关理论知识，里面的抛硬币游戏，你非常幸运，第一次进行这个实验就连抛了 20 次正面，你进行了很多次这个实验才得到了这个记录，这就会导致错误的预估，改进的方式是请 10 位不同的人进行这项实验，这样就可以观察到更多的样本数据，降低出现上述情况的概率，这就是 HLL 算法的核心思想。
 HLL实现原理的话，当输入一个元素的时候，会把元素通过Jenkins hash function（当然这里也可以用其他hash算法）转成哈希值然后再转换成01表示的二进制数据。
有了bit位值后，里面的0和1就可以表示某个事件结果的两种状态，例如正反面。有这些就可以找出每个位串上第一个最晚1出现的位置并且记录下来，并且来根据这个1来估算这些哈希值中不重复的个数。
例如有集合为[010, 100, 001], 集合中元素的第一个 1 出现的位置分别为 2, 1, 3，可以得到里面最大的值为 3，故该集合中第一个1出现的最晚的位置为 3因为每个位置上出现1的概率都是 1/2，所以我们可以做一个简单的推断，该集合中有 8 个不重复的元素。
这种简单的推断计算出来集合的基数值是有较大的偏差，为了减少误差，HLL 通过多次的进行试验来减少误差，HLL设计者使用了分桶的思想。
如上图，该 hash 值的后 10 位的 bit 值是 0000001001，转成十进制是 9，对应第 9 号桶，而该值第一个1出现的位置是第6位，比原先 9号桶中的数字大，故把 9 号桶中的数字更新为 6。
为什么是取后10位，这是是取决于结果的精确度，HLL算法的精度就越高，HLL(10) 有 1024(2^10) 个桶，HLL(16)有 65536(2^16) 个桶，桶的个数越多，所占用的空间也会越大。
其实我这里写的还是比较简单的，省略了一些细节，真实的 HLL 算法的完整描述见上图，这边的重点是计算桶中平均数时使用调和平均数，调和平均数的优点是可以过滤掉不健康的统计值，使用算术平均值容易受到极值的影响。
例如：求一个Google L3员工和一个 Google L6 的评价工资，如果使用传统的算法计算，计算两个相加然后/个数。</description>
    </item>
    
    <item>
      <title>Bernoulli Experiment</title>
      <link>http://blog.ibyte.me/post/bernoulli-experiment/</link>
      <pubDate>Mon, 25 Oct 2021 14:15:01 +0800</pubDate>
      
      <guid>http://blog.ibyte.me/post/bernoulli-experiment/</guid>
      <description>一个关于统计学的问题  说Hyperloglog之前，我得先写一些它的由来。设想一下，某某地方举办了一场技术交流活动，工作人员需要统计一下这个活动当天有多少的人参加？这个需求就是一个简单的统计的需求，解决方法有很多种，例如：活动举办方在会场门口设置一个签到处，每来一个参会者记录一下，最后统计一下人数就可以了。这是一个很简单问题，当时某一天作为开发的我，接到一个来自产品需求，要我统计一下在双十一1天内的某一个页面的UV（Unique Visitors）？，那么问题来了？怎么解决？
了解这个问题之前先说一下uv统计标准：独立访客UV指不同的用户，通过互联网访问同一个网页或产品的独立触发用户数。
假设一个场景: 今天爸爸、妈妈、儿子三人通过三个账号访问了某宝网页，则UV=3， 这里需要提一点：独立UV是按浏览器cookie为依据。只要cookie不清楚，3个人在0:00—24:00内用同一个浏览器不同的账号登陆，只会算作一个UV。
如图3个不同账户但是通过同一台电脑的浏览器访问的，如果默认以cookie作为标准的话，没有清理cookie的话，那么只会算一个uv。
那么有开发经验的肯定会说简单啊，用hashmap或者用set集合&amp;hellip;看似是一个简单问题，问题虽不难，但当参与问题中的变量达到一定数量级的时候，再简单的问题都会变成一个难题。假设日活用户达到百万或千万以上级别的话，我们采用 HashMap 的做法，就会导致程序中占用大量的内存，并且都是在并行的操作记录，还可能要考虑锁颗粒度问题，显然有经验的老司机会直接否决🙅这种方案。
   HyperLogLog  看了什么的问题，那有木有什么好的解决方案？ 有肯定是有的：B+ tree，Bitmap，在redis中就有造好的轮子的HyperLogLog概率数据结构算法，在redis中使用也就是3个api的事情：pfadd、pfcount、pfmerge。但是想深挖下去这个东西属实有点复杂，会涉及到一些数学上的东西，正好笔者我也看了看实现，顺便就写了这篇文章，HyperLogLog这个是由下面👇这个肥宅在他的论文中提出的，对可能这就是国外搞学术的大佬吧，不过可惜的是大佬在 2011年3月22日就去世了，不过他留下的HyperLogLog还是很值得研究的。
HLL的特点就是能花较低的内存占用统计海量的数据，但是缺点也有代码实现比较难，有一定的误差，如果要统计的100%的准确性，还是要考虑其他方案或者通过数学计算算出误差值（负载因子）。
在redis实现的HyperLogLog中能用12k内存就能统计2^64个数据，我表示很震撼。。。怎么做到的？？？
挖槽这是怎么做到的？看了一下那篇论文：http://algo.inria.fr/flajolet/Publications/FlFuGaMe07.pdf
   伯努利试验证明  在说HLL之前得先了解一下伯努利试验!
 伯努利试验是数学概率论中的一部分内容，它的典故来源于抛硬币游戏。 实验的内容：在同一个条件下重复地、各次之间相互独立地进行的一种试验，但是实验的结果只要两种结果，每次实验结果只会是两种结果中一个，然后在相同条件下重复做n次的试验称为n次独立重复试验，独立性是只各次试验的结果不会受其他实验结果的影响。
 次数较少的实验是没有意义的，只要当实验次数达到一定数量，就和微积分一样，短时间是看不出来差异的，但是如果把时间线拉长，那么差异就出来了。
实验过程： 硬币拥有正反两面，一次的上抛至落下，最终出现正反面的概率各自都是50%，假设一直抛硬币，直到它出现正面为止，我们记录为一次完整的试验，间中可能抛了一次就出现了正面，也可能抛了n次才出现正面，不管抛出多少次，只要出现了正面，就记录为一次试验。
重复不断这个过程，假设这个多次为n次，就意味着出现了n次的正面。假设每次伯努利试验所经历了的抛掷次数为k，第一次伯努利试验，次数设为k1，以此类推，第n次对应的是kn，在实验过程中肯定会出现抛出n才能出现一次正面，那么称这个为k_max，代表抛了最多的次数。
经过反复实验得出结果：
 N次实验抛出的次数不会大于k_max N次实验最少有一次的次数是k_max  当有了这些结论之后，发现在n和k_max中存在估算关联：n = 2^(k_max)，当然需要大量的数据和实验次数证明，如果需要深入挖掘其中的奥秘，那么还会涉及到数学中的概率和统计的方法才能推导和验证这种关联关系。。。。
1 2 3 4  第一次: 抛了3次出正面，此时 k=3，n=1 第二次: 抛了2次出正面，此时 k=2，n=2 第三次: 抛了6次出正面，此时 k=6，n=3 第n 次：抛了20次出正面，此时我们估算， n = 2^20   看上面的实验如果套用这个估算关系公式的话，那么结果是：上面例子中实验组数共3组，那么 k_max = 6，最终 n=3，我们放进估算公式中去，明显： 3 ≠ 2^6不成立的，但是证明了数据次数越少，意义就不大，发挥不了作用，就存在一定的误差值。</description>
    </item>
    
    <item>
      <title>Virtual Memory</title>
      <link>http://blog.ibyte.me/post/virtual-memory/</link>
      <pubDate>Fri, 08 Oct 2021 20:40:55 +0800</pubDate>
      
      <guid>http://blog.ibyte.me/post/virtual-memory/</guid>
      <description>什么是逻辑内存  什么是逻辑内存？把物理内存通过程序进行虚拟化出来的内存映射，这就称之为逻辑内存或虚拟内存。内存对于程序来说非常重要，当然大部分现在如果你不搞操作系统或者一些特定领域的开发一般很少了解。内存对于计算机来说非常宝贵的东西，现在的程序员只会在这些基础之上进行开发东西，内存管理是交给操作系统进行管理的。
   内存管理  对于操作系统来说怎么分配内存，怎么去给每个不同程序分配内存，管理数据，怎么隔离内存，怎么不让程序a去访问程序b的内存，内存不够了会怎么样？这些问题都是需要操作系统解决的。
假设计算机内存是4GB，这里需要运行4个程序，如图：
如图内存只要4GB但是需要运行的程序内存总和还要大于4GB，只能满足ABC这3个程序的运行，这时需要运行D那么就要等着，操作系统回收内存再去看看有木有合适的位置满足运行条件。
但是这又出现一个新的问题，如图：
当操作系统回收了A和C程序所占用的内存时，发现这块内存不是连续的，而D程序需要一块连续的内存才能正常跑起来，这里问题就是为什么需要有virtual memory的原因之一了。
   Virtual Memory  有了这些使用上的问题，然后就会出现了Virtual Memory这种技术，把物理加一层映射，而映射这一层就是现在的Virtual Memory，下面我画了一张图：
通过这种方式把物理内存虚拟化成一个虚拟化的内存，从而达到高效的利用，虚拟地址抽象不能在应用的运行过程中造成明显的开销，也不会占用过多的物理内存资源，有效的动态规划利用物理内存。虚拟内存可以完全把不同程序的内存隔离开来，让程序无法访问到其他程序的内存，安全性高。透明性，程序开发者也无限关系程序在运行的时候内存是否够用，感觉不到虚拟内存的存在。
虚拟内存划分规则有很多种，例如分段机制和分页机制这里我也没有打算写，后面有空再写写分段和分页区别和缺页异常和内存页替换策略，怎么在物理内存不够用的情况下去解决这个问题等...。
   其他  如果对相关内容感兴趣，可以去看看上海交大陈海波教授的操作系统课程，地址：https://ipads.se.sjtu.edu.cn/courses/os/，当然这个部分内存只支持内网访问。</description>
    </item>
    
    <item>
      <title>Go Slice Expansion Mechanism</title>
      <link>http://blog.ibyte.me/post/go-slice-expansion-mechanism/</link>
      <pubDate>Sun, 26 Sep 2021 23:06:08 +0800</pubDate>
      
      <guid>http://blog.ibyte.me/post/go-slice-expansion-mechanism/</guid>
      <description>切片是Golang里面一个复合数据类型，可以把它看做为一个可变长度的数组，和动态数组一样，在创建的时候我们可以指定容量大小，如果不够了，它还能指定扩容，基本的crud没有什么可说的本篇文章将写写切片底层的实现。
   slice struct  其实slice在底层就是一个struct声明一个结构体，结构如下：
1 2 3 4 5  type slice struct { array unsafe.Pointer len int cap int }   slice本身一个结构体里面包含了array和len和cap成员变量，array是一个指针指向真正存储数据的内存头元素地址，len记录着当前实际的元素个数，cap记录当前切片的容量，如上图所示。
   append expansion  在说append扩容机制之前，先看一个下面的题目，最终输出什么？？
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  package main import ( &amp;#34;fmt&amp;#34; ) func main() { array := [...]int{1,2,3,4,5} s1 := append(array[:3],array[4:].</description>
    </item>
    
    <item>
      <title>Computer Science Learning</title>
      <link>http://blog.ibyte.me/post/computer-science-learning/</link>
      <pubDate>Fri, 24 Sep 2021 21:38:31 +0800</pubDate>
      
      <guid>http://blog.ibyte.me/post/computer-science-learning/</guid>
      <description>本页面是本博主个人平时整理的一些计算机科学专业视频学习资料，整理放在下面方便大家浏览，希望对大家有帮助。
   资料名称 链接     数据库基础 哔哩哔哩视频   数据结构基础 哔哩哔哩视频   操作系统基础 上海交通大学-陈海波    </description>
    </item>
    
    <item>
      <title>Often Used Structured Query Languages</title>
      <link>http://blog.ibyte.me/post/often-used-structured-query-languages/</link>
      <pubDate>Tue, 21 Sep 2021 18:52:13 +0800</pubDate>
      
      <guid>http://blog.ibyte.me/post/often-used-structured-query-languages/</guid>
      <description>概 述  在开发过程中，作为一个crud boy来说会使sql来操作数据库增删改查是必不可少的，这篇文章将写写日常开发中常用的sql语句。数据是一个抽象的的定义，所谓的数据库就是把一些元数据通过特定规律整理到一起管理起来，方便通过他特定DDL，DCL，DML这些特定的语句来方便管理数据。
 DDL (data definition languages) 方便开发者通过SQL来定义存储的数据格式，组织数据。 DML (data manipulation languages) 允许用户对数据进行create、delete、insert、updated操作。 DCL (data control languages) 可以来管理用户对数据的访问操作权限，例如检索，更新，删除 &amp;hellip;     查询语句  在日常开发过程中查询数据是应用场景最多的，查询就要使用select关键字，如下：
1  select * from tableName;   例如有一个表结构如下：
上面是查询整张表的所有字段数据，如果需要筛选就需要制定其field name如下：
1  select name as &amp;#34;用户名&amp;#34; from TableName;   并且上面使用了as关键字并进行了别名，查询出来数据就按照别名进行展示。
如果想查询指定的数据,就要使用where子语句，如下：
1  select * from TableName where ID = 1;   那么上的SQL语句查询出来的结果就是上图中ID等于1的一条数据。
如果需要整理结果集去重复的话在前面加入distinct关键字，就可以去重了。
   查询过滤  有时候我们通过条件查询到的数据需要进行其他筛选，这个时候我们就要使用到limit或者order by进行过滤或者重新整理了。</description>
    </item>
    
    <item>
      <title>Golang Database Reverse Engineering</title>
      <link>http://blog.ibyte.me/post/golang-database-reverse-engineering/</link>
      <pubDate>Thu, 16 Sep 2021 22:49:08 +0800</pubDate>
      
      <guid>http://blog.ibyte.me/post/golang-database-reverse-engineering/</guid>
      <description>相信大家在开发的过程中会去编写一些数据库表对应的model，工作比较重复并且低效，本文将介绍笔者写的一个工具可以根据数据库表生产对应的model逆向工程工具。
   什么是s2s？  s2s (sql to structure)是一款命令行数据库逆向工程工具，它可以通过数据库表生成对应的Java、Go、Rust结构体（class），后面将陆续支持更多的语言。
   配置数据库源  s2s依赖于你的数据库，所以需要你配置好你的数据库连接信息，以便s2s会正常的运行。配置信息方法很简单你只需要在你的环境变量中加入以下信息即可。
推荐使用开发环境的下的root用户登录，因为工具需要information_schema表的权限。
1 2 3 4 5  #s2s 命令的数据库信息 export s2s_host=&amp;#34;127.0.0.1:3306&amp;#34; export s2s_user=&amp;#34;root&amp;#34; export s2s_pwd=&amp;#34;you db password&amp;#34; export s2s_charset=&amp;#34;utf8&amp;#34;   windows的配置此电脑-&amp;gt;属性-&amp;gt;高级系统设置-&amp;gt;环境变量，Mac和Linux则在~/.profile或者~/.zshrc中添加以上配置信息即可。
   使用方法    你可以克隆下载本代码库，然后如果你的电脑上已经安装好了go的编译器那么就进入主目录即可使用go build命令编译生成二进制程序文件。
  如果你觉得麻烦即可在下面列表中找到你对应的平台架构下载对应的二进制可执行文件到电脑上，如果你想在系统上随意调用你则只需要把s2s的安装目录放入你的环境变量中。
  目前对Rust部分数据类型支持不够友好，不过不耽误使用，目前被生成的数据库表名格式必须为user_info这样的snake case这种格式！！后面会考虑修复这个bug。
     平台 地址     Windows-x64 s2s-windows-x64.zip   Mac-x64 s2s-darwin-x64.</description>
    </item>
    
    <item>
      <title>Rust Lifetime</title>
      <link>http://blog.ibyte.me/post/rust-lifetime/</link>
      <pubDate>Thu, 16 Sep 2021 22:47:17 +0800</pubDate>
      
      <guid>http://blog.ibyte.me/post/rust-lifetime/</guid>
      <description>lifetime寿命  Rust中的每一个引用都有一个有效的作用域，生命周期就是为这个作用域服务的，大部分生命周期编译器可以推断出来，可以是隐式的。但是如果在某些情况下编译器就无法正常推断出来了，需要我们自己手动标注，标注生命周期语法就是&#39;a这样的语法。
   为什么需要生命周期？  例如下面例子就是在两个字符串切片里面查找最长的那个并且返回！
1 2 3 4 5 6 7 8  // &amp;#39;a 是指3个引用的作用域生命周期要一致 fn find_long_str&amp;lt;&amp;#39;a&amp;gt;(x: &amp;amp;&amp;#39;astr,y: &amp;amp;&amp;#39;astr)-&amp;gt; &amp;amp;&amp;#39;astr{ifx.len()&amp;gt;y.len(){x}else{y}}  上面我就加注了生命周期标识符，如果不加编译器会报错，原因是因为我们这个函数引用的是外部的变量，不能确定引用的变量是否已经被销毁了，那这样就是悬垂引用！
1 2 3 4 5 6 7 8 9 10  letstr1=String::from(&amp;#34;Hello&amp;#34;);letstr2;letresult;{//let str2 = String::from(&amp;#34; World!&amp;#34;); str2=String::from(&amp;#34; World!&amp;#34;);result=find_long_str(str1.as_str(),&amp;amp;str2);}// 这里借用检测就提示 引用了已经销毁的资源了 println!(&amp;#34;{}&amp;#34;,result);  加了生命周期标识符之后，如果我把let str2 = String::from(&amp;quot; World!&amp;quot;);取消注释放在一个内部作用域里面定义，那么这时调用 find_long_str编译器就会报错，因为我在下面出了作用域还使用了find_long_str返回的结果，而这个结果可能就是str2的内容， 使用这个是违反了所有权规则的，str2离开内部作用域就被销毁了。
在标注生命周期fn find_long_str&amp;lt;&#39;a&amp;gt;(x: &amp;amp;&#39;a str, y: &amp;amp;&#39;a str) -&amp;gt; &amp;amp;&#39;a str之后编译器就知道输入参数和返回参数生命周期是要一致的，并且返回值生命周期肯定是取生命周期最短的那个的。</description>
    </item>
    
    <item>
      <title>Rust Box Pointer</title>
      <link>http://blog.ibyte.me/post/rust-box-pointer/</link>
      <pubDate>Thu, 16 Sep 2021 22:46:21 +0800</pubDate>
      
      <guid>http://blog.ibyte.me/post/rust-box-pointer/</guid>
      <description>什么是智能指针？  在Rust中智能指针有很多种，用大白话说就是一个数据结构，实现了一些特殊的trait从而达到某种特性和功能，然后去管理某块内存上的数据，相当于一个盒子一样包装一层，这篇文章将介绍一下Box。
   Deref  Deref这个trait只要实现了它那么你自定义的结构体，就可以使用*解引用拿取你结构体里面所包装的数据，熟悉Rust都知道*是用来解引用的，下面的I32Box(i32)就是实现了Deref从而达到直接使用*拿取出内部的数据。
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  usestd::ops::Deref;struct I32Box(i32);// impl Deref implDerefforI32Box{type Target=i32;fn deref(&amp;amp;self)-&amp;gt; &amp;amp;Self::Target{&amp;amp;self.0}}implI32Box{fn new(v:i32)-&amp;gt; Self{I32Box(v)}}fn main(){letn=I32Box::new(1024_i32);println!(&amp;#34;{}&amp;#34;,*n);}     Drop  Rust里面有drop函数，也有一个Drop的trait同样如果自定义的结构体实现这个Drop，那么就你变量离开作用域的时候执行自定义Drop的方法的逻辑。
1 2 3 4 5  implDropforI32Box{fn drop(&amp;amp;mutself){println!(&amp;#34;啊，挂了，I32Box({:?})被清理了！&amp;#34;,self.0)}}  例如为I32Box实现Drop的drop函数，运行：
1 2 3 4 5 6 7  Compiling playground v0.</description>
    </item>
    
    <item>
      <title>Traffic Restrictions</title>
      <link>http://blog.ibyte.me/post/traffic-restrictions/</link>
      <pubDate>Thu, 16 Sep 2021 22:45:00 +0800</pubDate>
      
      <guid>http://blog.ibyte.me/post/traffic-restrictions/</guid>
      <description>前 言   在开发高并发系统时，我们可能会遇到接口访问频次过高，为了保证系统的高可用和稳定性，这时候就需要做流量限制，你可能是用的 Nginx 这种 Web Server 来控制请求，也可能是用了一些流行的类库实现。限流是高并发系统的一大杀器，在设计限流算法之前我们先来了解一下它们是什么。
    限 流  限流的目的是通过对并发访问请求进行限速，或者对一个时间窗口内的请求进行限速来保护系统，一旦达到限制速率则可以拒绝服务、排队或等待、降级等处理。通过对并发（或者一定时间窗口内）请求进行限速来保护系统，一旦达到限制速率则拒绝服务（定向到错误页或告知资源没有了）、排队等待（比如秒杀、评论、下单）、降级（返回兜底数据或默认数据）。
如 图:
如图上的漫画，在某个时间段流量上来了，服务的接口访问频率可能会非常快，如果我们没有对接口访问频次做限制可能会导致服务器无法承受过高的压力挂掉，这时候也可能会产生数据丢失，所以就要对其进行限流处理。
限流算法就可以帮助我们去控制每个接口或程序的函数被调用频率，它有点儿像保险丝，防止系统因为超过访问频率或并发量而引起瘫痪。我们可能在调用某些第三方的接口的时候会看到类似这样的响应头：
1 2 3  X-RateLimit-Limit:60//每秒60次请求X-RateLimit-Remaining:22//当前还剩下多少次X-RateLimit-Reset:1612184024//限制重置时间  上面的 HTTP Response 是通过响应头告诉调用方服务端的限流频次是怎样的，保证后端的接口访问上限。为了解决限流问题出现了很多的算法，它们都有不同的用途，通常的策略就是拒绝超出的请求，或者让超出的请求排队等待。
一般来说，限流的常用处理手段有：
 计数器 滑动窗口 漏桶 令牌桶     计数器   计数器是一种最简单限流算法，其原理就是：在一段时间间隔内，对请求进行计数，与阀值进行比较判断是否需要限流，一旦到了时间临界点，将计数器清零。 这个就像你去坐车一样，车厢规定了多少个位置，满了就不让上车了，不然就是超载了，被交警叔叔抓到了就要罚款的，如果我们的系统那就不是罚款的事情了，可能直接崩掉了。
  可以在程序中设置一个变量 count，当过来一个请求我就将这个数 +1，同时记录请求时间。 当下一个请求来的时候判断 count 的计数值是否超过设定的频次，以及当前请求的时间和第一次请求时间是否在 1 分钟内。 如果在 1 分钟内并且超过设定的频次则证明请求过多，后面的请求就拒绝掉。 如果该请求与第一个请求的间隔时间大于计数周期，且 count 值还在限流范围内，就重置 count。  代码实现:
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66  package main import ( &amp;#34;log&amp;#34; &amp;#34;sync&amp;#34; &amp;#34;time&amp;#34; ) type Counter struct { rate int //计数周期内最多允许的请求数  begin time.</description>
    </item>
    
    <item>
      <title>Web Safe Csrf</title>
      <link>http://blog.ibyte.me/post/web-safe-csrf/</link>
      <pubDate>Thu, 16 Sep 2021 22:43:44 +0800</pubDate>
      
      <guid>http://blog.ibyte.me/post/web-safe-csrf/</guid>
      <description>概 述  随着互联网的高速发展，信息安全问题已经成为企业最为关注的焦点之一，而前端又是引发企业安全问题的高危据点。在移动互联网时代，前端人员除了传统的 XSS、CSRF 等安全问题之外，又时常遭遇网络劫持、非法调用 API 等新型安全问题。当然，浏览器自身也在不断在进化和发展，不断引入 CSP、Same-Site Cookies 等新技术来增强安全性，但是仍存在很多潜在的威胁，这需要我们技术人员不断对系统进行“查漏补缺”。
   废话少说，先看问题  为了模拟问题我这边用go写了2个服务端的代码，正常交易系统的API，当然这里只是为了演示漏洞利用，代码比较简单如下:
transaction.go 内容:
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86  package main import ( &amp;#34;errors&amp;#34; &amp;#34;fmt&amp;#34; &amp;#34;github.</description>
    </item>
    
    <item>
      <title>Golang Web Session Implement</title>
      <link>http://blog.ibyte.me/post/golang-web-session-implement/</link>
      <pubDate>Thu, 16 Sep 2021 22:41:38 +0800</pubDate>
      
      <guid>http://blog.ibyte.me/post/golang-web-session-implement/</guid>
      <description>概 述  大家都知道 session 是web应用在服务器端实现的一种用户和服务器之间认证的解决方案，目前 Go 标准包没有为 session 提供任何支持，本文我将讲解session的实现原理，和一些常见基于session安全产生的防御问题。
当然有人可能看了会抬杠，说现在大部分不是前后端分离架构吗？对，你可以使用JWT解决你的问题。但是也有一些一体化web应用需要session，所以我准备造个轮子。自己造的轮子哪里出问题了，比别人更熟悉，有bug了，还不用求着别人修bug,自己修就好了，呵呵哈哈哈，当然这几句话有点皮😜。
   需 求  我觉得一名好的程序员，在写程序之前应该列一下需求分析，整理一下思路，然后再去写代码。
 支持内存存储会话数据 支持分布式redis会话存储 会话如果有心跳就自动续命30分钟（生命周期） 提供防御：中间人，会话劫持，会话重放等攻击     工作原理  首先必须了解工作原理才能写代码，这里我就稍微说一下，session是基于cookie实现的，一个session对应一个uuid也是sessionid，在服务器创建一个相关的数据结构，然后把这个sessionid通过cookie让浏览器保存着，下次浏览器请求过来了就会有sessionid，然后通过sessionid获取这个会话的数据。
   代码实现  都是说着容易，实际写起来就是各种坑，不过我还是实现了。
少说废话，还是直接干代码吧。
 依赖关系  上面是设计的相关依赖关系图，session是一个独立的结构体， GlobalManager是整体的会话管理器负责数据持久化，过期会话垃圾回收工作♻️，storage是存储器接口，因为我们要实现两种方式存储会话数据或者以后要增加其他持久化存储，所以必须需要接口抽象支持，memory和redis是存储的具体实现。
storage接口  1 2 3 4 5 6 7 8 9  package sessionx // session storage interface type storage interface { Read(s *Session) error Create(s *Session) error Update(s *Session) error Remove(s *Session) error }   storage就9行代码，是具体的会话数据操作动作的抽象，全部参数使用的是session这个结构的指针，如果处理异常了就即错即返回。</description>
    </item>
    
    <item>
      <title>Redis Distributed Lock</title>
      <link>http://blog.ibyte.me/post/redis-distributed-lock/</link>
      <pubDate>Thu, 16 Sep 2021 22:37:49 +0800</pubDate>
      
      <guid>http://blog.ibyte.me/post/redis-distributed-lock/</guid>
      <description>前 言  谈到分布式应用那必然离不开分布式锁🔐的问题，分布式锁在分布式应用中应用广泛，本文就讲讲基于redis实现的分布式锁的一些问题。
   锁  可能各位coder接触最多的还是在多线程的环境下，为了保证一个代码块在同一时间只能由一个线程访问情况，例如下图：
对于单进程的并发场景，可以使用编程语言及相应的类库提供的锁，如Java中的 synchronized 语法以及 ReentrantLock，Golang中的sync包下面的mutex，Rust中的async_std::sync::Mutex，避免并发问题，这实际上是本地锁的方式。
   分布式锁  但是现在流行的分布式架构，在分布式环境下，如何保证不同节点的线程同步执行呢？或者共享资源怎么上锁呢？？？
在将应用拆分为分布式应用之前的单机系统中，对一些并发场景读取公共资源时如扣库存，卖车票之类的需求可以简单的使用同步或者是加锁就可以实现，但是应用分布式了之后系统由以前的单进程多线程的程序变为了多进程多线程，这时使用以上的解决方案明显就不够了。
一般业界有几种解决方:
 基于 DB 的唯一索引 基于 Memcached的 add 命令 基于 Zookeeper 的临时有序节点 基于 Redis 的NX EX 基于Chubby粗粒度分布式锁服务     Redis的坑你填了几个？   如果在分布式场景中，实现不同客户端的线程对代码和资源的同步访问，保证在多线程下处理共享数据的安全性，就需要用到分布式锁技术，我就来写写基于Redis的一些坑😁。
 在分布式时，在程序中修改已有数据时，需要先读取，然后进行修改保存，此时很容易遇到并发问题。由于修改和保存不是原子操作，在并发场景下，部分对数据的操作可能会丢失，本地锁无法在多个服务器之间生效，这时候保证数据的一致性就需要分布式锁来实现。
Redis 锁主要利用 Redis 的 setnx 命令实现，
 加锁命令：SETNX key value，当键不存在时，对键进行设置操作并返回成功，否则返回失败,KEY 是锁的唯一标识，一般按业务来决定命名。 解锁命令：DEL key，通过删除键值对释放锁，以便其他线程可以通过 SETNX 命令来获取锁。 锁超时：EXPIRE key timeout, 设置 key 的超时时间，以保证即使锁没有被显式释放，锁也可以在一定时间后自动释放，避免资源被永远锁住。  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  // 伪代码实现 fn main(){letkey: &amp;amp;&amp;#39;staticstr=&amp;#34;sync_lock&amp;#34;;ifup_lock(key,1)==1{// 设置超时 expire(key,30)// .</description>
    </item>
    
    <item>
      <title>Alibaba Hot Ring Perceive</title>
      <link>http://blog.ibyte.me/post/alibaba-hot-ring/</link>
      <pubDate>Thu, 16 Sep 2021 22:27:07 +0800</pubDate>
      
      <guid>http://blog.ibyte.me/post/alibaba-hot-ring/</guid>
      <description>👨‍💻‍：直奔话题 HotRing，少说废话。   去年阿里的Tair团队发表过一篇论文HotRing: A Hotspot-Aware In-Memory Key-Value Store，这篇论文里面讲述了阿里的最快的KV分布式存储引擎核心HotRing的技术与实现，我看了他们的数据显示其引擎吞吐性能可达600M ops/s，这个速度比传统的KVS系统2.58倍的性能提升，于是我就抱着好奇去看看了那篇论文然后就有这篇文章。
    👨‍💻‍： 有问题就有故事？  一个新东西或者技术出现，那就说明老的那套方案技术存在某种缺陷或者满足不了某种的特殊需求了&amp;hellip;&amp;hellip;
问题动机就是：某一时刻流量上来了，系统扛不住了&amp;hellip; 只要说到高并发，相信这个回答占大多数。HotRing的出现也是这个原因，真的是面向问题编程😜。。。挖槽，真的验证这句话了。。。
这张图就是传统的kvs架构图，不管你中间的那一层用什么，在某些时刻，缓存系统迎来巨大的访问量（双11秒杀），可能存在访问倾斜(什么？你问我什么是访问倾斜，好吧你不适合编程。😜)，大多数访问集中在极少数数据上（例如微博热点事件）。
对于集群级别的热点监测倍受重视，如一致性 Hash，某个节点上的数据被读写访问次数远远高出其他节点，对此类行为没有很好的监测到。
还有单机热问题上没有优化，例如计算机内存的KV存储热点和查询速度没有优化。
一个好的存储这些事情都是必须去做的，从上到下，从下到上，每层都在解决一部分问题，总和就是一个大的问题，如果你不能把问题拆成小问题，那你那个问题不一定能解决的好，例如CPU Cache、LevelDB的设计。
   👨‍💻‍：HotRing 干了什么？？？  现有技术，内存 KVS 对时延要求是很高的，一个在并发中无锁的数据结构尤为重要，然后就是一个能让热点数据能自我跑到最容易访问的地方提高读取速度的。
 热点是动态变化的，如何检测，如何转移?  这个问题是逃不开的，在HotRing的设计，论文中提到了一个概念就是有序环。
它做了什么？看得懂上图就能看出来，他把传统的Map的底层的存储链表+数组结构换成了一个环形链结构，然后通过head自由的控制指向哪一个节点。
传统的结构可能你的数据在最后一层或者数据非常多，程序只能通过head顺序查找，时间复杂度O(n)
而环形的结构，避免多次遍历希望把热点数据放在冲突链前面，传统的则需要不断修改节点，需要不断移动节点，必须从头节点开始，到尾节点终止。移动头指针的情况下，会导致一些节点无法被访问。
移动热点不方便，需要把中间节点移动到头节点去，移动链表中的节点很复杂且难以做到无锁并发，就上图这个操作就要3步。
改成环链，Head可以指向任意节点，即从任意节点开始遍历，然后还能对数据进行排序。
为什么有序???  这个就属于环链的特点了，没有终结点，如果查找值不存在，无法判断何时终结。改成有序的了，可以根据前后项的关系判断是否终结本次查询。
 前驱节点 &amp;lt; 待查找节点 &amp;lt;后驱节点 miss 前驱节点 &amp;gt; 后驱节点 &amp;amp;&amp;amp; 待查找节点 &amp;lt; 后驱节点 miss 前驱节点 &amp;gt; 后驱节点 &amp;amp;&amp;amp; 待查找节点 &amp;gt; 前驱节点 miss 待查找节点 == 节点 K hit  排序的过程就是利用key排序可以解决这个问题，若目标key介于连续两个item的key之间，说明为read miss操作，即可终止返回。由于实际系统中，数据key的大小通常为10~100B，比较会带来巨大的开销。哈希结构利用tag来减少key的比较开销。</description>
    </item>
    
    <item>
      <title>Compound Data Type of Rust</title>
      <link>http://blog.ibyte.me/post/compound-data-type-of-rust/</link>
      <pubDate>Thu, 16 Sep 2021 22:25:30 +0800</pubDate>
      
      <guid>http://blog.ibyte.me/post/compound-data-type-of-rust/</guid>
      <description>概 述   好久没有更新rust相关的内容了，更新一波Rust的内容，本篇讲介绍一下Rust中的复合数据类型。
    Composite Type  复合数据类型是一种数据类型，它可以原始的基本数据类型和其它的复合类型所构成， 构成一个复合类型的动作，又称作组合。
本文讲介绍一下在Rust中有tuple、array、struct、enum几个复合类型。
   tuple  tuple即元组，元组类型是由多个不同类型的元素组成的复合类型，通过()小括号把元素组织在一起成一个新的数据类型。元组的长度在定义的时候就已经是固定的了，不能修改，如果指定了元素的数据类型，那么你的元素就要对号入座！！！否则编译器会教训你！
例子：
1 2 3 4 5 6 7 8 9 10 11  fn main(){// 指定数据类型 lettup_type:(i8,i32,bool)=(21,-1024,true);// 解构元素 let(one,two,three)=tup_type;// 二维的元组 lettup_2d:(f64,(i8,i32,bool))=(3.1415927,(one,two,three));println!(&amp;#34;tup_2d = {:?}&amp;#34;,tup_2d);// 索引 println!(&amp;#34;π = {:?}&amp;#34;,tup_2d.0);}  元组的访问方式有好几种，通过下标去访问，也可以使用解构赋值给新的变量去访问，但是不支持迭代器去访问。
1 2 3  forvintup_2d.1.iter(){println!(&amp;#34;{}&amp;#34;,v)}  1 2 3 4 5 6 7 8 9 10 11 12 13  Compiling playground v0.</description>
    </item>
    
    <item>
      <title>Rust Falsework Desgin</title>
      <link>http://blog.ibyte.me/post/rust-falsework-desgin/</link>
      <pubDate>Thu, 16 Sep 2021 22:22:45 +0800</pubDate>
      
      <guid>http://blog.ibyte.me/post/rust-falsework-desgin/</guid>
      <description>前言   断断续续看了半个月的Rust书，学了点基础，不知道做什么好？？于是我想着用业余时间撸一个command line application的骨架。然后就有了https://github.com/auula/falsework这个项目，falsework可以帮助你快速构建一个命令行应用。这篇文章写写falsework的使用并在后部分介绍一下怎么实现的。
 看了看这张图，那我估计还在坡道起步。。
   导入依赖   https://crates.io/crates/falsework  在你的项目中添加依赖如下：
1 2  [dependencies] falsework = &amp;#34;0.1.3&amp;#34;      快速构建  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45  usestd::error::Error;usefalsework::{app,cmd};fn main(){// 通过falsework创建一个骨架 letmutapp=falsework::app::new();// 应用元数据信息 app.</description>
    </item>
    
    <item>
      <title>Golang Channel Desgin</title>
      <link>http://blog.ibyte.me/post/golang-channel-desgin/</link>
      <pubDate>Thu, 16 Sep 2021 22:19:16 +0800</pubDate>
      
      <guid>http://blog.ibyte.me/post/golang-channel-desgin/</guid>
      <description>相信大家在开发的过程中经常会使用到go中并发利器channel，channel 是CSP并发模型中最重要的一个组件，两个独立的并发实体通过共享的通讯channel进行通信。大多数人只是会用这么个结构很少有人讨论它底层实现，这篇文章讲写写channel的底层实现。
   channel  channel的底层实现是一个结构体，源代码如下:
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  type hchan struct { qcount uint // total data in the queue  dataqsiz uint // size of the circular queue  buf unsafe.Pointer // points to an array of dataqsiz elements  elemsize uint16 closed uint32 elemtype *_type // element type  sendx uint // send index  recvx uint // receive index  recvq waitq // list of recv waiters  sendq waitq // list of send waiters  // lock protects all fields in hchan, as well as several  // fields in sudogs blocked on this channel.</description>
    </item>
    
    <item>
      <title>Golang Memory Escape Analysis</title>
      <link>http://blog.ibyte.me/post/golang-memory-escape-analysis/</link>
      <pubDate>Thu, 16 Sep 2021 22:07:09 +0800</pubDate>
      
      <guid>http://blog.ibyte.me/post/golang-memory-escape-analysis/</guid>
      <description>前 言  很多时候为了更快的开发效率，大多数程序员都是在使用抽象层级更高的技术，包括语言，框架，设计模式等。所以导致很多程序员包括我自己在内对于底层和基础的知识都会有些生疏和，但是正是这些底层的东西构建了我们熟知的解决方案，同时决定了一个技术人员的上限。
在写C和C++的时候动态分配内存是让程序员自己手动管理，这样做的好处是，需要申请多少内存空间可以很好的掌握怎么分配，但是如果忘记释放内存，则会导致内存泄漏。
Rust又比👆上面俩门语言分配内存方式显得不同，Rust的内存管理主要特色可以看做是编译器帮你在适当的地方插入delete来释放内存，这样一来你不需要显式指定释放，runtime也不需要任何GC，但是要做到这点，编译器需要能分析出在什么地方delete，这就需要你代码按照其规则来写了。
相比上面几种的内存管理方式的语言，像Java和Golang在语言设计的时候就加入了garbage collection也就runtime中的gc，让程序员不需要自己管理内存，真正解放了程序员的双手，让我们可以专注于编码。
   函数栈帧  当一个函数在运行时，需要为它在堆栈中创建一个栈帧（stack frame）用来记录运行时产生的相关信息，因此每个函数在执行前都会创建一个栈帧，在它返回时会销毁该栈帧。
通常用一个叫做栈基址（bp）的寄存器来保存正在运行函数栈帧的开始地址，由于栈指针（sp）始终保存的是栈顶的地址，所以栈指针保存的也就是正在运行函数栈帧的结束地址。
销毁时先把栈指针（sp）移动到此时栈基址（bp）的位置，此时栈指针和栈基址都指向同样的位置。
   Go内存逃逸  可以简单得理解成一次函数调用内部申请到的内存，它们会随着函数的返回把内存还给系统。下面来看看一个例子：
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  package main import &amp;#34;fmt&amp;#34; func main() { f := foo(&amp;#34;Ding&amp;#34;) fmt.Println(f) } type bar struct { s string } func foo(s string) bar { f := new(bar) // 这里的new(bar)会不会发生逃逸？？？  defer func() { f = nil }() f.</description>
    </item>
    
  </channel>
</rss>
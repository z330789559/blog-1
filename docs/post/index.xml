<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on 打码匠</title>
    <link>http://blog.ibyte.me/post/</link>
    <description>Recent content in Posts on 打码匠</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 05 Feb 2022 23:09:23 +0800</lastBuildDate><atom:link href="http://blog.ibyte.me/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Gws Design Instruction</title>
      <link>http://blog.ibyte.me/post/gws-design-instruction/</link>
      <pubDate>Sat, 05 Feb 2022 23:09:23 +0800</pubDate>
      
      <guid>http://blog.ibyte.me/post/gws-design-instruction/</guid>
      <description>各位新年快乐，这几天把之前写的一个session库重构了一下，顺便写一下升级和使用说明，如果你是一个萌新Gopher，可以看看源代码可以帮助你对Go编码技能提升，源代码也就不到800行左右，如果对你有帮助点一个star⭐️，谢谢🤗 ~
  https://github.com/auula/gws     GWS  Go&amp;rsquo;s web session library.
    介 绍  GWS是一个Go语言实现的WEB会话库，支持本地会话存储，也支持Redis远程服务器分布式存储，并且为了可扩展存储实现，预留工厂，方便开发者自定义实现存储来保存会话数据。
   安 装  开发者你只需要安装本库到你到项目里面，在你的项目里面执行下面命令即可安装：
1  go get -u github.com/auula/gws      使用示例  首先要声明一点，gws是支持多种存储介质保存session数据的，你可以自定义实现gws.Storage存储接口，来使用你自定义存储，接口代码如下:
1 2 3 4 5 6 7 8 9 10  // Storage global session data store interface. // You can customize the storage medium by implementing this interface.</description>
    </item>
    
    <item>
      <title>Distributed System CAP Theory</title>
      <link>http://blog.ibyte.me/post/distributed-system-cap-theory/</link>
      <pubDate>Wed, 26 Jan 2022 23:26:59 +0800</pubDate>
      
      <guid>http://blog.ibyte.me/post/distributed-system-cap-theory/</guid>
      <description>前言  在分布式系统中大部分应用程序的部署方式是分散的，系统有多个节点分散在个区域，相互之间通过网络进行通讯。一个计算请求可能需要经过集群中的多个节点，然后再返回给客户端，但是影响这个请求的可能是网络速度，部分节点宕机，无响应，响应过慢，来至脏数据交叉污染，这些问题就给分布式系统设计者带来许多挑战，例如：故障恢复和故障提取发现，节点存活检测等等&amp;hellip;
本人最近一个多月也在研究相关领域的论文资料，这么看下来，我总结几句话：该出问题的肯定会出问题，我们要做的就是把出错率降低，只要是个程序肯定就会有问题出现，毕竟程序这个东西是跑在计算机上的，数据也是动态的未知的，分布式更是依赖于网络，我们要做的就是在设计的时候考虑未来系统或者在出错或者出问题的时候怎么解决，把出错率降低，提高可用性。
   什么原因会引发故障？  首先需要明确的是，只有当单个节点的处理能力无法满足日益增长的计算、存储任务的时候，且硬件的提升（加内存、加磁盘、使用更好的CPU）高昂到得不偿失的时候，应用程序也不能进一步优化的时候，我们才需要考虑分布式系统。分布式系统容错性，个人认为是分布式系统设计中最重要的部分，打个比方，一个集群有一个节点出现问题，不可能说导致我整个系统都不能使用了。所以后面出现了一些分布式故障检测机制，可以根据故障类型来给出解决方案，然后出现了分布式数据一致性算法。
分布式系统中的一些数据一致性问题可以看成平时在写单机程序出现的并发和并行资源竞争问题一样，不知道大家有木有写过go语言，go语言在处理数据竞争或者说并行任务通讯的时候，采用了CSP来共享状态，也就是通过消息来共享程序里面的线程或者协程状态的设计，用消息来互相通讯，这是一个很棒👍🏻的设计。
在分布式系统设计中也是这样的，一个集群里面的节点通过网络互相来通讯，可是用网络怎么做到不丢消息？网络数据包顺序问题？响应速度问题，一个请求进来了，但是这个请求处理速度要依赖于集群中处理计算最慢那个节点的速度，然后在返回，这么一折腾，分布式系统坑太多了。。这里还没有说到分布式数据一致性问题，还是单单基于网络通信出现的问题和各个节点本身的问题。。。
1. 请求处理
大部分应用都是接到客户端发送的请求，然后处理请求，如果是简单的计算请求，可能就几毫秒就处理完成了，但是一个需要消耗时间的请求就需要耗费大量时间处理，一个请求还好，如果是大量的耗时请求，那么再小的问题，也随着请求流量增多，而加大。此时即使分布式系统，最终处理也是在每个计算节点上完成的，还是会落到当个节点上，然后汇总返回结果给客户端。
大家都知道生产者与消费者模型，在二者中间加一个缓冲区，来缓解当前处理系统压力，在分布式系统上，也采用这个设计，根据生产者生成数据速度，动态调整缓存队列大小，把请求处理管道化，也可以吸收一瞬间产生的巨大流量，也使得接收和处理在时间上分开。
2. 时间和数据包顺序
集群中的节点都是分开的，时间这个也会影响计算，可能A节点向B节点发送了2个数据包，但是这2数据包，达到B节点时发现顺序反了，换句话说就是这两个数据包需要按照前后顺序处理，可以在数据包里标记序号解决。
一些强依赖于时间的程序，就对时间问题有很高要求，可以抽出一个统一的时间签发领导节点，负责整个集群的时间管理工作，当然这是看完相关论文笔记，如果读者对这些感兴趣，你们直接研究吧，这块确实水深啊。
3. 级联和关联故障
相信大家如果做个微服务开发，或者对微服务有了解的应该知道熔断机制，就和电流里面的保险丝一样，起到保护作用。关联故障分布式系统逃不开的话题，由于系统是集群模式，一个节点出现的故障，可能就会联系到与它关联的节点上，从一个系统传播到另外一个系统，如果不做处理，最后可能就会污染整个分布式系统来至崩溃。当一台服务器无响应的时候，可能一个客户端会无数次的循环发出请求，可以设置请求重试时间窗口来消处抖动。
   故障监测  虽然单个节点的故障概率较低，但节点数目达到一定规模，出故障的概率就变高了，分布式系统需要保证故障发生的时候，系统仍然是可用的，这就需要监控节点的状态，在节点故障的情况下将该节点负责的计算、存储任务转移到其他节点上继续处理操作。
为了系统更加可用，分布式系统引入故障检测是必须的，在程序运行的时候我们开发者是根本不知道什么时候会出错，运行时候的程序里面的数据都是动态的。故障检测器几乎是每个分布式系统必不可缺的组件，能帮助整个系统去分辨其他节点是挂了还是卡住了，还是崩溃了。。。。由于是分布式系统靠着网络通讯，所以最常见的办法就是通过心跳包来解决，但是我不知道大家有没有了解过网络传输协议，tcp和udp，或者大家在开发的时候，两个独立运行的系统为什么要用去kafka去传递消息，而不是自己用tcp去写一个服务器和客户端互相传递消息？网络这个东西本来就是不可靠的，我发送一个数据包，然后这个数据包丢了，或者举个比较奇葩的例子：一个盗墓的把你两个机房之间的光缆挖断了。。。都是不可靠的。。
说到了kafka，正好写一下，kafka消息也是基于网络传输，只是他作为一个消息中间件，在原有的网络协议上，增加了一些特殊的策略去保证消息在网络上传递是可靠的。
言归正传，故障检测这个在分布式系统也是一个大坑，常见也就是一个心跳包去检查节点，默认可能节点设置的有心跳包生命周期的，一个心跳包发送出去了，可能我需要3秒内得到回复，但是巧好网络问题，这个心跳包是5秒钟回复的，此时其他节点生命周期过了，认为这个节点可能挂了，但是其实又没有挂。。。然后有出现了一种无超时的心跳检测机制，每个节点维护者一个相邻节点信息列表和计数器，每个心跳包都包含着计数器的值。
还有一种广播机制，类似上学的时候，老师发了一张签名表，然后从门口座位第一个同学哪里开始传，直到全部签好名传回为止，这种方法就类似于gossip协议。
   KVBase何去何从？  一开始打算用raft算法去解决的，但是发现我要的是节点存储的时候都是一部分数据，而不是全量复制，或者是说不是强一致的数据，所以打算用gossip协议和一致性哈希去做分布式系统初步骨架，我目前来看我觉得kvbase注重最终一致性，类似于Apache Cassandra的架构那样。
   总结  上面这些常见的问题，当然上面我还没有详细写，如果真的要深挖的话，我估计都能自己写一本书出来了，系统设计就要在设计的时候考虑到这些问题，装上熔断、退避、验证、协调、数据版本一致等等故障检测机制，这也开发者带来很大的挑战，当然我个人是这么认为的，如果有朋友对这块感兴趣可以去看看一本书《数据库系统内幕》，本文我就是对阅读这本书后的总结和笔记整理，分布式系统这如果真的想做健全可用真的是一件很有挑战的事情，因为改出错地方肯定会出错！！！</description>
    </item>
    
    <item>
      <title>Open Source My Opinion</title>
      <link>http://blog.ibyte.me/post/open-source-my-opinion/</link>
      <pubDate>Fri, 21 Jan 2022 18:05:44 +0800</pubDate>
      
      <guid>http://blog.ibyte.me/post/open-source-my-opinion/</guid>
      <description>最近一两年开源软件这个词，已经被炒着很热了，热度很高了，国内也涌现了一批靠着开源做商业化的公司，也有不少风险投资也在投资这些公司。开源软件真的能赚钱吗？开源软件的商业模式在哪里？如何运营一个开源社区？怎么去推广自己的项目？和闭源软件的优势在哪里？最近看到一些朋友在讨论开源软件一些问题，我个人泡一些技术社区也好久了，于是我想写写我对开源软件一些看法。
 我第一台个人电脑是我5年级的时候，我是一个典型的Z时代，我爸给我买的，我现在使用的电脑是第5台电脑了，读者可以算一下间隔多少年了，当时我对电脑键盘都不会用，虽然学校有微机课，但是中国的微机课就是用来看电影的，大家懂得。。。然后我爸有一个同事以前在新疆开网吧的，然后特地让他来教我玩电脑&amp;hellip;反正我也不知道当时我们怎么想的&amp;hellip;鬼知道呢&amp;hellip;我记得当时那个叔叔还对我爸爸说了一句话你不想你儿子以后好好上学了吗？现在就给他买电脑？&amp;hellip;我当时还是上的私立双语学校&amp;hellip;现在想想确实我没有好好上学&amp;hellip;当然我们班上全是这样的&amp;hellip;可能有点机缘吧&amp;hellip;当然前面几年放假回家就是在家打游戏，第一次写代码当时用的是E语言，应该在初二的时候，反正没有人教我，就单纯自己写，加了一些QQ群学习所谓的什么黑客技术和泡国内最大的黑客论坛黑基网，还写了一些锁机病毒和盗号程序，魔改一下别人写好的外挂插件&amp;hellip; 上传到一些游戏外挂论坛上，然后别人中马了，电脑就被锁机了，然后他们会主动联系我，然后我就找他们要赎金，让他们给我充Q币。
其中还有人要报警，吓得我赶紧把密码给他了。。。。现在看看真是没有什么技术含量，脚本小子行为，反编译就能看到明文密码，或者进入pe模式清楚密码，因为这些实质后面还认识了接触到了一些头部安全的公司技术人员，为什么说这个，因为这个方式你可以和业界技术大佬交流，虽然是通过互联网。
我第一次接触开源软件是2016年的时候，不对可能之前使用过开源软件，在这之前我玩过Kali Linux里面有开源软件，搞渗透测试，只是我当时不知道而已。当时我还在使用Java语言，当时给我上课的老师都是从一些企业过来的讲师，给我们班上讲一些业务开发课，当时了解到Spring和MyBatis这几个框架，老师说这些都是开源软件，还说Spring框架早期作者是一个玩音乐出生的，我当时听了就呵呵😑笑了笑。
使用了这些框架做开发后，相比自己纯手撸JDBC和Servlet，改改配置，撸一点核心逻辑代码，就可以跑起来一个Web Service，真是太爽了，当时年纪还很小只是觉得挺好玩的，我以后要是写出这么牛逼的框架就好了，当时对开源理解就是程序的代码是公开的，然后就是可以随便使用。。。现在看看，第一条应该是对的，后面那个要看项目使用的什么开源协议了。
真正参入一个开源项目是2017年了，当时遇到了我大师兄，也就是Blade框架的作者，后面在他带领之下开始同性交友了，当然我写软件完全是靠兴趣，和玩一样，把写代码当成乐趣。
   什么是开源软件？  开源软件是通过特定类型的许可证发布的软件，这种许可证能让最终用户合法地使用其源代码，此类许可证有许多种，但通常开源软件必须符合以下条件：
 以源代码形式提供，无需额外费用：这意味着用户可以查看组成该软件的代码并对其进行所需的任何更改。 源代码可重新用于其他新软件：这意味着任何人都可以获取源代码并利用它来分发自己的程序。     开源软件是免费的吗？  不是，开源不等于免费！！！ 具体来说这与开源软件采用的许可协议/许可证有关，开源软件允许使用者修改源代码，但是不允许修改版权信息，除非授权方宣布放弃版权，一些开源软件可能会采取双许可证，对于商业应用往往需要购买商业许可证并支付费用。典型的例子就是红帽RedHat，任何人都可以免费拿到红帽Linux的源代码，但是要将源代码转换成可运行的代码，则需要相应的专业知识，例如大家经常使用的centos就是合法化的换掉商标的红帽。
   开源软件真的能赚钱吗？  能，肯定是能，但这个能得看社区和运营和用户群体了，开源有做到上市的公司，典型例子就是ElasticSearch，无心插柳柳成荫，这里面还有一个故事，Elasticsearch创始人Shay Banon跟随自己的新婚妻子来到伦敦时，他还是一个待业的工程师，妻子想通过学习成为一名厨师，而他则想为妻子开发一个方便搜索菜谱的应用，经过一系列机缘巧合的经历后，Shay Banon成功基于Apache Lucene打造了一个易用的、高性能、实时、分布式搜索引擎Elasticsearch，通过开源，Elasticsearch算是开源介独角兽公司了，公司市值达到150亿美元。
国内做开源的公司例如deepin算是国内最早从事这方面的公司了，他们的CTO王勇前辈（当然目前已经离职了）也算是国内最早接触开源软件的，有兴趣可以去看看这个帖子：
 https://www.deepin.org/zh/2012/08/13/interview-with-andy-stewart-from-linux-deepin-team/  还有pingcap这种newsql公司，如果你留意观察一下这些公司，pingcap公司会积极参加一些技术分享的活动，和组织一些技术交流活动，还在线上推出相关的技术认证课程，这个模式就有点像早些年的Oracle的技术认证一样，我记得上学的时候老师说如果Java学的不好，还可以去专攻数据库，写得一手好SQL安装安装什么 Oracle 11g啥的，玩溜了，也可以有饭吃的，考一个Oracle认证或者加一个红帽认证，当然我算班上Java玩的溜的了。
从上面这些例子来看，一项技术火不火，热不热，社区和用户群，还有开发者和官方运营起到了很大作用，有了用户还怕没有钱赚吗？
包括现在Github开通了资助功能，一个个人开发者也会收到他们粉丝发来的打赏，早期Github没有这个功能，开发者可能需要在仓库里面粘贴PayPal收款码，现在方便多了。
这里插一嘴，玩一个梗，有人问怎么看一个技术火不火？我的回答是看看市面上，学校开没开相关课程，如果开了，那这个技术肯定火🔥。。。
   和闭源软件的优势在哪里？  软件吞噬世界，开源吞噬软件，这句话正在被应验，相信大家做开发，日常开发中使用的技术栈肯定有开源软件。前不久飞书官方不是有一个视频吗？我看了好像是他们产品经理在上面说，飞书字节跳动投入了多少多少产研，人力物力，可我想说的是，你们飞书没有用开源软件吗？如果没有开源软件我相信开发投入会更大。
以前可能把源代码闭源是科技公司的核心竞争力，产品的核心竞争力，或者出一套企业级解决方案，搞商业授权，例如Oracle企业级开发全家桶，Oracle Linux和Oracle数据库、JavaEE一整套，和这个对应就是微软的C#、SQLServer、Windows Server IIS。
相对于上面这些PHP就是LAMP和LNMP就可以跑起来了，虽然我没有写过PHP但是我还是了解一点的，这些方案就是选择用开源的软件作为基础设施来跑PHP。
随着nodejs的出现，PHP目前市场占有率也在下降，nodejs没有出现前，哪里来的大前端？那个时候javascript能干什么？现在是PHP能做的，JavaScript写起来都比他溜。。
这就是nodejs出现颠覆前端，就像你2000年想过现在手机是这个样子吗？那个时候有一台扣机就不错了，只是没有人出来颠覆而已，就像乔布斯掏出iPhone 那一刻，全场woc，woc&amp;hellip; 直接颠覆了诺基亚时代。。。。有了nodejs出现，才诞生了vue和react这样的项目。
以往很多公司把闭源当做核心竞争力，不过个人认为你做的是客户端应用这种还好，闭源是一种选择。但是现在开发者那么多，有能力开发者那么多，闭源不一定是你的核心竞争力，我举一个例子，例如Google他们的Google三驾马车的代码没有开源，但是他们公布了设计论文，然后有人通过他们的论文实现了Hadoop，产生了Hadoop这样的顶级项目，Hadoop实现就是基于他们的论文实现的，Hadoop目前在大数据领域不知道被多少公司使用，所以文章也是一种开源。
   个人怎么推广自己项目？怎么运营？  第一点你的项目使用什么技术实现的，例如Go语言实现的，那就一定找到官方社区然后去泡社区，因为社区里面专业相关的领域开发者是最多的，一定要敢于说话，保持自己的观点，有争论也别怂，你可以通过讲道理方式解决。其次就是一定要利用好自媒体平台，他们的算法会帮助你的项目找到对应的目标用户，然后就是不要盯着国内这块地方，项目要定为国际化的项目，多去泡国外的社区，注册官方推特，去hacker news上发帖子。</description>
    </item>
    
    <item>
      <title>Pretty Terminal</title>
      <link>http://blog.ibyte.me/post/iterm2-zsh/</link>
      <pubDate>Fri, 07 Jan 2022 23:44:05 +0800</pubDate>
      
      <guid>http://blog.ibyte.me/post/iterm2-zsh/</guid>
      <description>概述  很多人看到我的终端觉得很酷，问我怎么配置的？今天这篇文章我就来教教大家怎么配置一个骚气的终端，先让大家看看配置好的终端效果图吧：
是不是很骚气？？？
   颜色配置   iTerm2：号称 Mac 下最好的终端工具。 Zsh：一款强大的终端工具，自定义可玩性非常高，如果你是Linux用户也可以安装。  1. 第一步安装iTerm2，如果你已经安装了brew软件包管理工具，可以执行下面命令：
1  brew install iterm2   如果你没有安装brew工具，你可以执行下面命令进行安装：
1  /bin/bash -c &amp;#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&amp;#34;   但是这里有问题的，那就是大家懂得网络问题，可能dns污染，不能正常访问到GitHub，如果你是Mac用户把你的dns设置为8.8.8.8，如下图：
配置这个dns之前，你得先有🪜梯子你懂得，在大陆编程，我觉得程序员第一件事就是要解决网络问题，我感觉80%问题都是网络引起的问题。。
2. 下载和配置主题，首先去设置背景颜色，位置在 iTerm2 -&amp;gt; Preferences -&amp;gt; Profiles -&amp;gt; Terminal。
把配色设置为xterm-256color。
3. 虽然 iTerm2 有 7 种自带的配色，像我这种对颜值要求高的，肯定是满足不了我需求了，既然有像我这种有需求的，那肯定有人提供这种高颜值的主题，于是有大神在GitHub开源了一个仓库https://github.com/mbadolato/iTerm2-Color-Schemes，里面收录各种主题配色方案。
通过终端回到你用户根目录，执行下面命令：
1 2 3  mkdir ~/.iterm2 &amp;amp;&amp;amp; cd ~/.iterm2 git clone https://github.com/mbadolato/iTerm2-Color-Schemes   然后导入主题到你终端里面：</description>
    </item>
    
    <item>
      <title>Linux File System Inode</title>
      <link>http://blog.ibyte.me/post/linux-file-system-inode/</link>
      <pubDate>Sun, 26 Dec 2021 22:18:31 +0800</pubDate>
      
      <guid>http://blog.ibyte.me/post/linux-file-system-inode/</guid>
      <description>文件系统  现在计算机数据存储功能是必不可少的，大家的图片和视频都是数据，但是这些数据是怎么保存在硬盘上的呢？我不知道大家有没有想过？存储介质分类有很多种，例如：机械硬盘、固态硬盘、闪存&amp;hellip;这些都是物理存储介质，怎么让数据保存在里面？不丢失？那么多数据又是怎么在组织起来管理的呢？这时就要说说文件系统了。
文件系统就是操作系统给我们抽象一种中间层让我们和物理磁盘可以打交道，文件系统就是负责把用户的文件存到磁盘硬件中，因为即使计算机断电了，磁盘里的数据并不会丢失，所以可以持久化的保存文件。
   怎么工作的？  文件系统怎么工作的？无论底层存储介质是磁盘还是SSD，都被该层抽象为 Block 的概念。文件系统在初始化时，会先在挂载的块存储上的第一个位置创建一个 Super Block，文件在抽象之前，每个文件是有元数据信息的，例如：文件名字、文件大小、创建时间、访问时间、归属者、所在组等&amp;hellip;构成元数据信息，好比现实中一本书，书的各种信息。
Linux 最经典的一句话是：一切皆文件!，不仅普通的文件和目录，就连块设备、管道、socket 等，也都是统一交给文件系统管理的，但是这些信息要怎么组织到磁盘上，而且文件可能随着时间推移大小也随着变化，文件可能被拷贝到不同物理介质上，这就给文件系统设计者带来一个大问题？
文件系统会把底层存储的物理硬盘设备，抽象成单单固定大小的块，块区，那这么多快和分区要怎么组织去管理？后面出现了一种基于inode 索引节点的文件组织管理方式。
   文件组织管理方式  硬件磁盘会被划分成固定大小块，存储块大小可能是512字节的sector然后8个组成一个4KB的块，这个看具体划分方式了，文件也是对应的这么划分存储的，这么设计的时候那就可以把每个块加上唯一编号，也可以看成文件的名字，然后把块建立一个索引，但是大部分存储文件的都是以mb作为单位的，例如：一部电影可能有4GB大小，这就给快存储设计带来设计挑战，一个4GB的电影就需要100万个4kb的块，但是每个块上还有一个唯一的文件名称或者编号，需要占用8字节的空间，这么一搞那么就需要另外的8mb记录，又带了来这样的问题？
有了这些问题，文件系统设计者引入一个叫index node的数据结构，文件数据被存储不同块里面，文件的元数据信息就会被存储在inode里面。
inode会被存储在inode table里面的，inode也就是inode table里面条目，inode table 包含该文件系统中所有文件的列表，inode table 中的各个 inode 项具有唯一的编号。
inode table记录这个inode number对应文件所对应的metadata，如上图所示👆🏻，这样我们就方便管理文件元数据了，每个 inode 都有一个号码，操作系统用 inode 号码来识别不同的文件，这里值得重复一遍，Unix/Linux 系统内部不使用文件名，而使用 inode 号码来识别文件。对于系统来说，文件名只是 inode 号码便于识别的别称一样。
我们在系统调用的时候，发生了什么？实际上，系统内部将这个过程分成三步：
 系统找到文件名对应的 inode number 通过 inode number，获取 inode 信息 根据 inode 信息，找到文件数据所在的 block，读写数据。     inode 怎么工作的？  通过上面的了解，得知了每个文件都对应一个inode，然后管理文件的时候把所有文件块存储在inode里面但是这种不是没有问题的，随着时间文件不断加大，inode就要在创建的时候就预留足够好的空间，才能保证后面文件的各种变化，例如我未来存储文件是4GB，我现在就要去把inode占用大小设置为8mb显然这个不能充分发挥磁盘利用率。如果inode分配小了，未来文件变大了，inode不够用，各种问题。。。</description>
    </item>
    
    <item>
      <title>Bigcache Golang Garbage Collection</title>
      <link>http://blog.ibyte.me/post/bigcache-golang-garbage-collection/</link>
      <pubDate>Fri, 24 Dec 2021 20:39:56 +0800</pubDate>
      
      <guid>http://blog.ibyte.me/post/bigcache-golang-garbage-collection/</guid>
      <description>为什么有这篇文章  某一天在群里摸鱼的时候，看到群里有人问go map的空间回收问题，把截图贴上吧： 其实一位群友发出的问题引起了我注意，他的问题是：go的map的值调用了delete函数是不是不会立即删除？当然这个问题如果研究过或者深入go的内存分配或者说有了解过go的gc应该知道，这个问题答案。
关于是分段锁的应用和怎么去优化gc带来的影响，有一个开源项目bigcache在这方面做的比较好。
   BigCache的设计   官方作者介绍：快速，并发，基于内存的缓存库，由于是嵌入式库也省去了网络上的开销，完全基于本地内存，能保存大量数据项的同时并且对go语言的garbage collection进行了优化。
 对并发锁的颗粒度减小，并且对gc优化它怎么做的？
 分段锁 数据二进制存储，避免让gc去嵌套扫描 加速并发访问 避免高额的GC开销  官方在他们blog上列出了，他们当时写这个库的需求：
 处理10k rps (写5000，读5000) cache对象至少存活10分钟 更快的响应时间 POST请求的每条 JSON 消息,一有含有ID，二不大于500字节 POST请求添加缓存后，GET能获取到最新结果  Go map 的问题
我在网上看到资料有提到一个问题：https://github.com/golang/go/issues/9477
大致看了一下这个说了问题就是，Go 1.3和1.4RC1垃圾回收在扫描一个大的map时需要50-70ms时间，问官方怎么能有什么办法减小这个时间。
然后1.5版本，如果 map 的 key 或 value 中都不含指针， GC 便会忽略这个 map。
主要的也就两个结构体cache和cacheShard:
1 2 3 4  type cache struct { shards []*cacheShard // 块map 解决并发锁颗粒度问题  hash fnv64a // 哈希函数 }   源码中的哈希函数用的是fnv64a算法，这个算法的好处是采用位运算的方式在栈上进行运算，避免在堆上分配。</description>
    </item>
    
    <item>
      <title>KVBase storage engine Bitcask</title>
      <link>http://blog.ibyte.me/post/bitcask-kvbase/</link>
      <pubDate>Tue, 21 Dec 2021 18:14:23 +0800</pubDate>
      
      <guid>http://blog.ibyte.me/post/bitcask-kvbase/</guid>
      <description>天上飞的概念，也要落地的实现  Bitcask相信如果你了解或者做过存储相关的工作的，相信或多或少都了解过，它是由basho的bitcask论文所设计的key-value存储引擎，相关的应用有Riak这个数据库，Riak是以 Erlang 开发的一个高度可扩展的分布式数据存储，Riak的实现是基于Amazon的Dynamo论文实现的，说到Dynomo那是另外一篇论文的事情了（如果你想看中文版我上一篇文章就是翻译的Dynamo论文可以找找历史记录），Dynamo他们也根据自己的实际应用做了相应的优化，言归正传，这篇我还是围绕Bitcask存储引擎在KVBase落地的实现写写。
   Bitcask怎么工作的？  Bitcask是使用RAM 存储指向值的文件指针的哈希映射，索引用于高效写入的日志结构文件系统，二者组合而成的存储引擎，可能光文章描述可能很难理解，我下面画了几幅图。
Bitcask主要几个组件，我个人认为最重要也就3个组件：
 Index Map 全局索引映射 Active File 当前可写文件的指针 Compressor 负责脏数据压缩和整理进程  key会存储在内存中以便快速查找，所有的value都存储于磁盘中，这种方法特点是以追加的方式写磁盘，即写操作是有序的，这样可以减少磁磁盘的寻道时间，是一种高吞吐量的写入方案，在更新数据时，也是把新数据追加到文件的后面，然后更新一下数据的文件指针映射即可。
当写操作发生时，keydir被原子的更新，更新成新的文件上的位置，文件上的老数据还在磁盘上，新的读操作会使用新的keydir，如果在并发大量请求情况下，这个估计会出现性能瓶颈的地方之一，这个坑我会在后面说说怎么去优化。
Active File是在磁盘上的文件，每条数据对应数据格式如下图：
每条数据都一条一条以行的形式存储在数据文件里，如图上通过特殊的编码方式就可以得到key的大小和value的大小，然后就可以找到key的位置和value的内容然后反编码解码操作，数据在写入的时候是被编码成二进制的存储的，补充一下CRC是循环校验码，是数据通信领域中最常用的一种差错校验码，毕竟数据库里面的数据有时候需要在网络上进行传输。
   Bitcask缺陷   这种基于内存的索引的必然有问题的，例如机器重启，内存数据丢了，索引就无法对磁盘数进行映射，还有上面提到的全局的读写必须经过这个keydir进行定位到数据源，另外读取数据的依赖于操作系统内核的文件系统缓存，没有用户态实现，不可控。
  为了解决重启内存索引数据丢失的问题，Bitcask论文中提到了hint file文件，如果是直接扫描data文件然后来建立索引是一件非常耗时的工作，要调用操作系统打开多个数据文件，然后来回遍历记录，官方解决方案是在一部分data文件记录对应一个hint文件，扫描一个hint file相比去读取data文件要省事的多，里面记录如下：  第二个问题就是数据都是增删改查的，数据被删除是肯定会发生的，那么Bitcask作为一个只追加记录的引擎，怎么解决？随着记录不断增多，数据文件也会变得更大，为了节省空间，bitcask采用merge的方式剔除脏数据，merge期间会影响到服务的访问，如果想保留原始的数据文件，可以使用谷歌的Snappy压缩算法，在磁盘IO和cpu之间做一个权衡，以便使程序跑得更快，Snappy就是这样一种快速的数据压缩算法。对于一个核的i7处理器（64位模式），能达到250M/s以上的处理速度。  针对这个问题，我对KVBase采用的bit桶key标记删除策略，当被删除的key达到一定数量的时候（通过算法去判定是否需要），如果需要就会启动一个独立进程去访问为未被标记的数据，并且开始整理到新的数据文件中，在此期间服务依然可用，整理完成之后我只需要原子操作把内存映射的指针指向新的索引上，这个过程比bitcask论文用的整个合并数据文件要好得多，不会因为整理过程中整个服务被阻塞着，最多就是短暂消耗一点内存空间罢了，整理完成之后清理废弃内存数据和内存空间，这也是牺牲空间换取可用性和时间。我承认这个设计借鉴了Golang原生的map的扩容策略，如果你对go原生的map有了解的话，或者是看过go源代码的话，应该很清楚。
   KVBase的设计  要知道Bitcask是以追加的方式写磁盘，即写操作是有序的，这样可以减少磁磁盘的寻道时间，是一种高吞吐量的写入设计，写性能肯定是没有得话说的。但是读取数据是通过内存中索引进行查询读取，虽然key的定位是通过hash进行O(1)时间复杂度，但是如果数据存放在老数据文件里面，此时就要调用系统内核api去读取文件，会产生短暂的io开销。
为了解决读取性能的问题，我在KVBase上加了层cache并且这层cache是支持分布式的，为什么这么做？要知道数据存储问题底层的Bitcask引擎已经解决了，写入数据不就是为了数据不能丢吗？这个问题已经解决了，大部分写数据场景估计也不多，大部分都是读数据场景，所以加一层基于内存的cache为读取数据，提供保障，为了解决内存空间大小没有磁盘空间大的问题，引入一些内存淘汰算法，保证空间问题，当然具体哪个我还在考量，或者像redis那样从内存数据里面随机抽取一部分数据，如果超过4/1的数据即将过期，那么就重复抽样几次：
底层存储已经和cache被单独抽象出来了，以后就算换掉底层的存储引擎实现也不会影响上层cache，在KVBase我个人认为存储引擎只是为cache服务的，cache被抽象成了分布式的了，意味着后面我可以动态分配数据存储节点，这个就没有什么好讲的了。
   小结  如果这篇文章对读者有什么启发，那我很高兴，文章也是一种技术分享，如果你对这块感兴趣，有什么想吐槽的，我非常欢迎交流，实质来一起coding。</description>
    </item>
    
    <item>
      <title>Redis Cluster System Design</title>
      <link>http://blog.ibyte.me/post/redis-cluster-system-design/</link>
      <pubDate>Wed, 24 Nov 2021 14:37:48 +0800</pubDate>
      
      <guid>http://blog.ibyte.me/post/redis-cluster-system-design/</guid>
      <description>为什么会出现集群架构？  上一篇写到了redis哨兵模式，这种模式虽然能解决一些并发情况下的数据请求压力问题和可用性问题，主从替换以及故障恢复。哨兵模式只是redis架构在历史演变过程中一个的转折点，哨兵这种模式主从模式的数据都是全量同步的，也就是每个从节点上的数据都是主节点上的数据副本，大家想一想有木有问题？
是不是太浪费内存了？或者说可不可以把全量的数据拆解成部分数据，然后分布在不同节点上？那么这种有什么好处吗？？？
如果数据全部存储在一台机子上，全量同步的时候会时间过长，有时候出现了big key情况更是一个问题，网络一波动，本来传输正在进行中，一半断掉了，会增加同步失败的概率。分片的存储好处，是不是可以减低这些风险的，每个节点我存储一部分数据，实质我们可以交叉存储数据块，a节点存储b节点一个副本，b节点存储a节点一个副本，这样就算如果a当掉了，我们是不是可以去b里面拿取副本继续使用，当然这个我是自己的一些看法，我看hdfs就是这么设计的，当然redis没有这个交叉存储策略，但是笔者认为如果在key下功夫也可以达到的，把key以特定格式的命名的格式二次存储也能达到，只是说浪费一点内存吧了。
   存储模型  有木有想过为什么redis为什么是k:v这种存储模型，而不是直接设计成关系型数据库那样的？？熟悉redis应该都了解一点历史，使用key-value是有历史原因的，早期的时候，redis作者起初是为提高自己网站的读写并发能力而创建redis，而key-value字典结构的常规时间复杂度是O(1)，刚好满足作者的性能要求和业务数据结构需求。
redis大部分作用都是缓存数据的，大部分数据都是从db读取的，而且这些数据也不是db中全量的数据，只是一部分而已，没有高度组织化结构化数据，用一部分数据，怎么组成一个完整的关系模型？所以不需要结构化查询语言。
   Codis分片方案  正是redis作者这些历史原因，本来在设计之初可能也就是单机就可以满足作者自己的需求，但是没想到它写的这个软件被这么多人使用，影响力这么大，然后出现了主从模式，哨兵模式，直到本文要讲的集群模式。
 计算机科学领域的任何问题都可以通过增加一个间接的中间层来解决!
 Redis的集群也有演变历史，早期有Twemproxy是Twitter团队的开源的解决分片方案，还有Codis的方案值得注意这是一个国人开源的，也就是现在pingcap几位创始人，前豌豆荚团队的，不得不说屌爆了。
他们方案都有一个共同特点，都是通过一个proxy代理层进行的分片处理的，不知道有没有使用过Nginx的，使用过大家应该知道Nginx反向代理一些服务程序，例如Nginx和Tomcat组成一个反向代理负载均衡的集群，codis也是类似这样的方案。
Codis就是起着一个中间代理的作用，Codis内部维护一个key映射算法，客户端访问codis和直接访问redis没有区别的，因为codis实现了redis的通讯协议，协议这个东西很重要的，双方都商量好的怎么处理数据，做起事来，就很方便，大家懂得。。。。Codis是一个无状态的，所以可以增加多个Codis来提升QPS,同时也可以起着扩容的作用。
Codis怎么工作的？
  在Codis会把所有的key分成1024个槽，这1024个槽对应着的就是Redis的集群，这个在Codis中是会在内存中维护着这1024个槽与Redis实例的映射的，当然这个是可以配置，就看你的Redis的节点数量有多少，偏多的话，可以设置槽多一些。
  codis会先是把key进行CRC32 后，得到一个32位的数字，然后再hash%1024后得到一个余数，这个值就是这个key对应着的槽，这槽后面对应着的就是redis的实例。
  那么问题来了？我多个codis怎么解决槽位共享？
Codis把这个工作交给了ZooKeeper来管理，Codis节点会监听到ZooKeeper的槽位变化，会及时同步过来，如下图
这种方式有木有什么弊端？？？
如果是使用mset批量设置值，值是是不是被分散在各个节点上，还有如果rename的时候也是一个问题，不过我认为有一种解决方案在映射的时候给key生成一个唯一的uuid，我rename的只是key，而映射的时候使用uuid计算槽位，当然这些问题在后面的redis官方解决方案里面得到了解决。
   官方集群方案  说起官方集群解决方案，是redis作者在前面那些代理中间件出现之后发布的，我看他这个设计之后感觉应该借鉴了codis的设计方案，只是把集群槽位信息同步改成p2p然后通过gossip协议来解决了同步的问题，个人感觉都是都是redis在设计之初留下技术负债问题，早期的时候估计作者自己也没有打算写集群方案，被逼的，当然这个是我个人的想法和看法。
相对于 Codis 的不同，它是去中心化的，如图下面我画的图，该集群有4个 Redis 节点组成， 每个节点负责整个集群的一部分数据，每个节点负责的数据多少可能不一样，这4个节点相 互连接组成一个对等的集群，它们之间通过一种gossip协议相互交互集群信息。
当然我上面画的图太接近一致性哈希环了，但是设计思想是接近的，Redis Cluster 将所有数据划分为 16384 的 slots，比 Codis 的 1024 个槽划分得更为精细，每个节点负责其中一部分槽，槽位的信息存储于每个节点中，它不像 Codis存储在zookeeper中，相信大家都听说过p2p，不清楚自己去查吧。。
那么问题来了！客户端怎么获取不同节点中的key呢？
有木有想过，我客户端连接的是a节点，而我要的数据在b节点上，这个问题怎么解决的呢？redis在通讯协议上下了功夫！它借鉴了http协议的重定向的概念，大家的应该熟悉http中重定向协议吧，如下图：
客户端来连接集群时，它也会得到一份集群的槽位配置信息。这样当客户端要查找某个 key 时，可以直接定位到目标节点，这么一来就解决了数据重定向拿取问题，上面我说到了codis的rename问题，官方解决方案是可以让客户端把每个key设置一个tag然后指定到对应节点槽位上，想深入了解的自己看看一些资料吧，我这里主要讲大问题思路。</description>
    </item>
    
    <item>
      <title>How Redis Solves the CAP Problem</title>
      <link>http://blog.ibyte.me/post/how-redis-solves-the-cap-problem/</link>
      <pubDate>Wed, 10 Nov 2021 17:22:55 +0800</pubDate>
      
      <guid>http://blog.ibyte.me/post/how-redis-solves-the-cap-problem/</guid>
      <description>CAP理论  CAP理论是什么？ 可以这么说吧cap理论是分布式系统的奠基石吧，这个理论论述分布式系统设计的3个最大问题：
 一致性 (Consistency) 可用性 (Availability) 分区容错性 (Partition Tolerance)  什么是一致性？假设一个集群有2个节点A-1和A-2，这两个节点为主从关系，例如redis里面的主从模式。主节点负责写入数据，从节点负责从主节点同步数据到本机。
A-1和A-2 通过网络进行数据同步，要怎么保证两台机子上的数据一致的呢？这个就是数据一致性问题。
那什么是可用性？A-1和A-2两台机子做成一个集群，实际上真实情况可能更多，在集群中一部分节点故障后，集群整体是否还能响应客户端的读写请求，例如A-1挂掉以后，A-2会马上接管这些请求的，保持整体可用性。
那什么是分区容错性？一个集群都靠着网络进行通讯的，如果网络波动，这个集群中的部分节点网络和集群上的其他节点联通不了，那么数据一致性就很难保持了，网络一断开，那么那么整个集群可能被划分成多个独立的小分区了，如果在短时间内不能联通同步数据，这就是出现网络分区问题。
如上图 A-1和A-3网络连接出现了故障，那么这个集群就已经存在了网络分区问题！但是整体集群而看上去却好像是在一个可以运转正常，其他剩下的机器还能够正常运转满足系统需求，对于用户而言并没有什么体验上的影响。
所以在设计分布式系统时，CAP问题必会出现！除非你玩单机系统😜！怎么优雅解决问题，就看设计者了在A和C、P之前怎么做决策了。用这些问题我去套用在redis集群模式上，看redis集群是怎么解决CAP问题的。
   Redis哨兵  Redis早期版本，redis单机情况下即便性能强的一批，但是大规模数据或者海量请求下还是为支撑不了可用性和稳定性，后面redis官方有一个解决方案就是哨兵模式。
哨兵模式：上面提到了主从架构，主从切换技术方案是，当主服务器宕机后，需要手动把一台从服务器切换为主服务器，这中间就需要人工干预，例如大半夜收到报警，爬起来手动切换服务器？？？需要手动将 从节点 晋升为 主节点，同时还要通知 客户端 更新 主节点地址，显然这种问题怎么可能忍？大家都是程序员自己写个程序自动化不就好了，为什么要用这么笨方式。。。哨兵模式哨兵就是解决这个问题的，哨兵是一个独立的进程，作为进程，它会独立运行，其原理是哨兵通过发送命令，等待Redis服务器响应，从而监控运行的多个Redis实例，当有一个节点挂掉的时候，客户端连接其节点失败，就会向哨兵服务器询问新的节点地址然后继续使用，其他从节点也会跟着把主节点调整到新节点上，哨兵还会记录当前挂掉节点状态，还会定时进行查看如果复活了还会添加到哨兵集群里，从而达到无人干扰。
   增量同步&amp;amp;amp;全量同步  上面哨兵模式切换过程，提到了主从，那主从节点数据，怎么保证一致性的的？在redis集群情况下同步模式有两种：
 增量同步 全量同步  设想一下主从节点在某个网络情况出现问题波动，这是客户端正在朝着服务器端主节点，写数据，而从节点又在从主节点读取同步被写入的数据，这时网络一波动，那么就出现网络分区了。出现网络分区了，此时两个节点就处于独立的单机状态，如图：
正如上图所话的，主从在短时间内出现网络分区，redis的设计者针对这个方案提出了一个方案叫增量复制，如下图，当然这个图我之前讲go中的channel的时候画的，反正是同用的，所以懒得话了，直接拿过来用一用。
redis在每个主节点上分配一快ring也可以叫为buf，这个ring是里面就记录它自身大小元素个数的最近客户端执行命令记录，如果从节点短时间内恢复了链接，此时从节点直接去主节点的ring中拿取命令记录数据，然后在自己本机上跑一遍。
增量复制不是没有问题的，设想一下如果这个网络分区出现很长时间，这个ring容量是不是不够用，所有第二套解决方案出现了，全量同步这个过程是怎么样的？
直接把redis在内存里面的数据通过序列化然后通过网络发送到从节点，从节点在本地保存，然后全量加载到内存里面完成数据恢复，在全量同步的时候，可能也有其他命令漏掉，还会配上增量复制进行。
   小结  目前只说到Redis哨兵模式了，这个模式能解决一部分CAP问题，例如还有：节点动态添加，数据动态迁移。。。这些问题在下一篇文章写，也是经典的分而治之的应用。</description>
    </item>
    
    <item>
      <title>Why Is Redis High Performance</title>
      <link>http://blog.ibyte.me/post/why-is-redis-high-performance/</link>
      <pubDate>Thu, 04 Nov 2021 14:32:49 +0800</pubDate>
      
      <guid>http://blog.ibyte.me/post/why-is-redis-high-performance/</guid>
      <description>Redis的高性能怎么做到的？  Redis这个NOSQL数据库在计算机界可谓是无人不知，无人不晓。只要涉及到数据那么就需要数据库，数据库类型很多，但是NOSQL的kv内存数据库也很多，redis作为其中一个是怎么做到行业天花板的呢？是怎么做到高性能的呢？怎么做到高可用的呢？今天这篇八股文我就整理一些redis的设计写写，本篇还是偏关于高性能这一块。
   高效数据结构  Redis的数据库相比传统的关系数据库，在数据结构上也是比较特殊的，它的所有数据类型都可以看做是一个map的结构，key作为查询条件。
Redis基于KV内存数据库，它内部构建了一个哈希表，根据指定的KEY访问时，只需要O(1)的时间复杂度就可以找到对应的数据，而value的值又是一些拥有各种特性的数据结构，这就给redis在数据操作的时候提供很好的性能了。
   基于内存存储  相比传统的关系数据库，数据文件可能以lsm tree 或者 b+ tree形式存在硬盘上，这个时候读取文件要有io操作了，而redis在内存中进行，并不会大量消耗CPU资源，所以速度极快。
内存从上图可以看到它介于硬盘和cpu缓存中间的，相比硬盘查找数据肯定是快的，当然这里笔者个人见解上，如果关系型数据库把一些平凡操作的数据库也放置在内存中缓存，也会得到一些性能的提升，像操作系统里面缺页异常一样处理，把数据片段通过一些特殊算法缓存在内存里面，减少文件io的开销。
   io多路复用  传统对于并发情况，假如一个进程不行，那搞多个进程不就可以同时处理多个客户端连接了么？多进程是可以解决一些并发问题，但是还是有一些问题，上下文切换开销，线程循环创建，从PCB来回恢复效率较低。随着客户端请求增多，那么线程也随着请求数量直线上升，如果是并发的时候涉及到数据共享访问，有时候涉及到使用锁来控制范围顺序，影响其他线程执行效率。（进程在Linux也可以理解为线程，每个进程只是有一个线程，当然这里我上面写的进程，别纠结这些。。。）
线程是运行在进程上下文的逻辑流，一个进程可以包含多个线程，多个线程运行在同一进程上下文中，因此可共享这个进程地址空间的所有内容，解决了进程与进程之间通信难的问题，同时，由于一个线程的上下文要比一个进程的上下文小得多，所以线程的上下文切换，要比进程的上下文切换效率高得多。
像redis和Nginx这种应用就是单线程的程序，为什么他们能做到这么强的性能？首先看一个例子：
 Blocking IO   中午吃饭，我给餐厅老板说要一碗‘热干面’，然后我就在那边一直等着老板做，老板没有做好，我就一直在哪里等着什么也不做，直到‘热干面’做好。
 这个流程就是我们常说的Blocking I/O如图：
同步阻塞 IO 模型中，应用程序发起 read 调用后，会一直阻塞，直到内核把数据拷贝到用户空间。
 Non Blocking IO  切换一下常见：
 同样你中午吃饭，给餐厅老板说要一碗‘热干面’，然后老板开始做了，你每隔几分钟向老板问一下‘好了吗？’，直到老板说好了，你取到‘热干面’结束。
 同步非阻塞 IO 模型中，应用程序会一直发起read调用，等待数据从内核空间拷贝到用户空间的这段时间里，线程依然是阻塞的，直到在内核把数据拷贝到用户空间，通过轮询操作，避免了一直阻塞，取回热干面的过程就是内核把准备好的数据交换到用户空间过程。
综上两种模型，缺点都是差不多，都是在等待内核准备数据，然后阻塞等待，同样逃不开阻塞这个问题，应用程序不断进行I/O系统调用轮询数据是否已经准备好的过程是十分消耗CPU资源的。
I/O Multiplexing  还是之前那个例子：
 中午吃饭，给餐厅老板说要一碗‘热干面’，然后老板安排给下面的厨子做，具体哪个厨子做不知道，有好几个厨子，然后老板每隔一段时间询问下面的厨子有木有做好，如果做好了，就通知我来去取餐。
 IO 多路复用模型中，线程首先发起 select 调用，询问内核数据是否准备就绪，等内核把数据准备好了，用户线程再发起 read 调用，read 调用的过程（数据从内核空间-&amp;gt;用户空间）还是阻塞的。</description>
    </item>
    
    <item>
      <title>Redis Current Limiting</title>
      <link>http://blog.ibyte.me/post/redis-current-limiting/</link>
      <pubDate>Thu, 28 Oct 2021 21:41:23 +0800</pubDate>
      
      <guid>http://blog.ibyte.me/post/redis-current-limiting/</guid>
      <description>Current Limiting  在编写系统时候有时候我们的系统在设计的时候就已经估算到了最大请求负载了，如果大量的请求超过系统所能承受着的值时，那么系统可能随时挂掉，所有聪明程序员就想到了请求限流来控制系统的可用和稳定性。
   滑动窗口限流  滑动窗口算法将一个大的时间窗口分成多个小窗口，每次大窗口向后滑动一个小窗口，并保证大的窗口内流量不会超出最大值，这种实现比固定窗口的流量曲线更加平滑。
以系统限制用户行为为例子，比如一秒内进行某个操作5次，这种行为应该进行限制，滑动窗口就是记录一个滑动的时间窗口内的操作次数，操作次数超过阈值则进行限流。
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  public boolean isActionAllowed(String userId, String actionKey, int period, int maxCount) { // 生成唯一的key  String key = String.format(&amp;#34;hist:%s:%s&amp;#34;, userId, actionKey); long nowTs = System.currentTimeMillis(); // 使用管道  Pipeline pipe = jedis.pipelined(); pipe.multi(); // 添加当前操作当zset中  pipe.</description>
    </item>
    
    <item>
      <title>Hyperloglog</title>
      <link>http://blog.ibyte.me/post/hyperloglog/</link>
      <pubDate>Mon, 25 Oct 2021 14:18:04 +0800</pubDate>
      
      <guid>http://blog.ibyte.me/post/hyperloglog/</guid>
      <description>Hyperloglog  上一篇文章HLL基础部分介绍了HLL一些相关理论知识，里面的抛硬币游戏，硬币正反两面，如果用计算机里面的数据表示的话，完全可以使用0和1，1表示正面，0表示反面。
HLL常用于去重场景，HLL 算法需要完整遍历所有元素一次，而非多次或采样，算法只能计算集合中有多少个不重复的元素，不能给出每个元素的出现次数或是判断一个元素是否之前出现过，多个使用 HLL 统计出的基数值 可以融合。
HLL的空间只会和精度有关，下面的后面不同的数字代表着不同的精度，数字越大，精度越高，占用的空间也越大。
 上一篇文章HLL基础部分介绍了HLL一些相关理论知识，里面的抛硬币游戏，你非常幸运，第一次进行这个实验就连抛了 20 次正面，你进行了很多次这个实验才得到了这个记录，这就会导致错误的预估，改进的方式是请 10 位不同的人进行这项实验，这样就可以观察到更多的样本数据，降低出现上述情况的概率，这就是 HLL 算法的核心思想。
 HLL实现原理的话，当输入一个元素的时候，会把元素通过Jenkins hash function（当然这里也可以用其他hash算法）转成哈希值然后再转换成01表示的二进制数据。
有了bit位值后，里面的0和1就可以表示某个事件结果的两种状态，例如正反面。有这些就可以找出每个位串上第一个最晚1出现的位置并且记录下来，并且来根据这个1来估算这些哈希值中不重复的个数。
例如有集合为[010, 100, 001], 集合中元素的第一个 1 出现的位置分别为 2, 1, 3，可以得到里面最大的值为 3，故该集合中第一个1出现的最晚的位置为 3因为每个位置上出现1的概率都是 1/2，所以我们可以做一个简单的推断，该集合中有 8 个不重复的元素。
这种简单的推断计算出来集合的基数值是有较大的偏差，为了减少误差，HLL 通过多次的进行试验来减少误差，HLL设计者使用了分桶的思想。
如上图，该 hash 值的后 10 位的 bit 值是 0000001001，转成十进制是 9，对应第 9 号桶，而该值第一个1出现的位置是第6位，比原先 9号桶中的数字大，故把 9 号桶中的数字更新为 6。
为什么是取后10位，这是是取决于结果的精确度，HLL算法的精度就越高，HLL(10) 有 1024(2^10) 个桶，HLL(16)有 65536(2^16) 个桶，桶的个数越多，所占用的空间也会越大。
其实我这里写的还是比较简单的，省略了一些细节，真实的 HLL 算法的完整描述见上图，这边的重点是计算桶中平均数时使用调和平均数，调和平均数的优点是可以过滤掉不健康的统计值，使用算术平均值容易受到极值的影响。
例如：求一个Google L3员工和一个 Google L6 的评价工资，如果使用传统的算法计算，计算两个相加然后/个数。</description>
    </item>
    
    <item>
      <title>Bernoulli Experiment</title>
      <link>http://blog.ibyte.me/post/bernoulli-experiment/</link>
      <pubDate>Mon, 25 Oct 2021 14:15:01 +0800</pubDate>
      
      <guid>http://blog.ibyte.me/post/bernoulli-experiment/</guid>
      <description>一个关于统计学的问题  说Hyperloglog之前，我得先写一些它的由来。设想一下，某某地方举办了一场技术交流活动，工作人员需要统计一下这个活动当天有多少的人参加？这个需求就是一个简单的统计的需求，解决方法有很多种，例如：活动举办方在会场门口设置一个签到处，每来一个参会者记录一下，最后统计一下人数就可以了。这是一个很简单问题，当时某一天作为开发的我，接到一个来自产品需求，要我统计一下在双十一1天内的某一个页面的UV（Unique Visitors）？，那么问题来了？怎么解决？
了解这个问题之前先说一下uv统计标准：独立访客UV指不同的用户，通过互联网访问同一个网页或产品的独立触发用户数。
假设一个场景: 今天爸爸、妈妈、儿子三人通过三个账号访问了某宝网页，则UV=3， 这里需要提一点：独立UV是按浏览器cookie为依据。只要cookie不清楚，3个人在0:00—24:00内用同一个浏览器不同的账号登陆，只会算作一个UV。
如图3个不同账户但是通过同一台电脑的浏览器访问的，如果默认以cookie作为标准的话，没有清理cookie的话，那么只会算一个uv。
那么有开发经验的肯定会说简单啊，用hashmap或者用set集合&amp;hellip;看似是一个简单问题，问题虽不难，但当参与问题中的变量达到一定数量级的时候，再简单的问题都会变成一个难题。假设日活用户达到百万或千万以上级别的话，我们采用 HashMap 的做法，就会导致程序中占用大量的内存，并且都是在并行的操作记录，还可能要考虑锁颗粒度问题，显然有经验的老司机会直接否决🙅这种方案。
   HyperLogLog  看了什么的问题，那有木有什么好的解决方案？ 有肯定是有的：B+ tree，Bitmap，在redis中就有造好的轮子的HyperLogLog概率数据结构算法，在redis中使用也就是3个api的事情：pfadd、pfcount、pfmerge。但是想深挖下去这个东西属实有点复杂，会涉及到一些数学上的东西，正好笔者我也看了看实现，顺便就写了这篇文章，HyperLogLog这个是由下面👇这个肥宅在他的论文中提出的，对可能这就是国外搞学术的大佬吧，不过可惜的是大佬在 2011年3月22日就去世了，不过他留下的HyperLogLog还是很值得研究的。
HLL的特点就是能花较低的内存占用统计海量的数据，但是缺点也有代码实现比较难，有一定的误差，如果要统计的100%的准确性，还是要考虑其他方案或者通过数学计算算出误差值（负载因子）。
在redis实现的HyperLogLog中能用12k内存就能统计2^64个数据，我表示很震撼。。。怎么做到的？？？
挖槽这是怎么做到的？看了一下那篇论文：http://algo.inria.fr/flajolet/Publications/FlFuGaMe07.pdf
   伯努利试验证明  在说HLL之前得先了解一下伯努利试验!
 伯努利试验是数学概率论中的一部分内容，它的典故来源于抛硬币游戏。 实验的内容：在同一个条件下重复地、各次之间相互独立地进行的一种试验，但是实验的结果只要两种结果，每次实验结果只会是两种结果中一个，然后在相同条件下重复做n次的试验称为n次独立重复试验，独立性是只各次试验的结果不会受其他实验结果的影响。
 次数较少的实验是没有意义的，只要当实验次数达到一定数量，就和微积分一样，短时间是看不出来差异的，但是如果把时间线拉长，那么差异就出来了。
实验过程： 硬币拥有正反两面，一次的上抛至落下，最终出现正反面的概率各自都是50%，假设一直抛硬币，直到它出现正面为止，我们记录为一次完整的试验，间中可能抛了一次就出现了正面，也可能抛了n次才出现正面，不管抛出多少次，只要出现了正面，就记录为一次试验。
重复不断这个过程，假设这个多次为n次，就意味着出现了n次的正面。假设每次伯努利试验所经历了的抛掷次数为k，第一次伯努利试验，次数设为k1，以此类推，第n次对应的是kn，在实验过程中肯定会出现抛出n才能出现一次正面，那么称这个为k_max，代表抛了最多的次数。
经过反复实验得出结果：
 N次实验抛出的次数不会大于k_max N次实验最少有一次的次数是k_max  当有了这些结论之后，发现在n和k_max中存在估算关联：n = 2^(k_max)，当然需要大量的数据和实验次数证明，如果需要深入挖掘其中的奥秘，那么还会涉及到数学中的概率和统计的方法才能推导和验证这种关联关系。。。。
1 2 3 4  第一次: 抛了3次出正面，此时 k=3，n=1 第二次: 抛了2次出正面，此时 k=2，n=2 第三次: 抛了6次出正面，此时 k=6，n=3 第n 次：抛了20次出正面，此时我们估算， n = 2^20   看上面的实验如果套用这个估算关系公式的话，那么结果是：上面例子中实验组数共3组，那么 k_max = 6，最终 n=3，我们放进估算公式中去，明显： 3 ≠ 2^6不成立的，但是证明了数据次数越少，意义就不大，发挥不了作用，就存在一定的误差值。</description>
    </item>
    
    <item>
      <title>Virtual Memory</title>
      <link>http://blog.ibyte.me/post/virtual-memory/</link>
      <pubDate>Fri, 08 Oct 2021 20:40:55 +0800</pubDate>
      
      <guid>http://blog.ibyte.me/post/virtual-memory/</guid>
      <description>什么是逻辑内存  什么是逻辑内存？把物理内存通过程序进行虚拟化出来的内存映射，这就称之为逻辑内存或虚拟内存。内存对于程序来说非常重要，当然大部分现在如果你不搞操作系统或者一些特定领域的开发一般很少了解。内存对于计算机来说非常宝贵的东西，现在的程序员只会在这些基础之上进行开发东西，内存管理是交给操作系统进行管理的。
   内存管理  对于操作系统来说怎么分配内存，怎么去给每个不同程序分配内存，管理数据，怎么隔离内存，怎么不让程序a去访问程序b的内存，内存不够了会怎么样？这些问题都是需要操作系统解决的。
假设计算机内存是4GB，这里需要运行4个程序，如图：
如图内存只要4GB但是需要运行的程序内存总和还要大于4GB，只能满足ABC这3个程序的运行，这时需要运行D那么就要等着，操作系统回收内存再去看看有木有合适的位置满足运行条件。
但是这又出现一个新的问题，如图：
当操作系统回收了A和C程序所占用的内存时，发现这块内存不是连续的，而D程序需要一块连续的内存才能正常跑起来，这里问题就是为什么需要有virtual memory的原因之一了。
   Virtual Memory  有了这些使用上的问题，然后就会出现了Virtual Memory这种技术，把物理加一层映射，而映射这一层就是现在的Virtual Memory，下面我画了一张图：
通过这种方式把物理内存虚拟化成一个虚拟化的内存，从而达到高效的利用，虚拟地址抽象不能在应用的运行过程中造成明显的开销，也不会占用过多的物理内存资源，有效的动态规划利用物理内存。虚拟内存可以完全把不同程序的内存隔离开来，让程序无法访问到其他程序的内存，安全性高。透明性，程序开发者也无限关系程序在运行的时候内存是否够用，感觉不到虚拟内存的存在。
虚拟内存划分规则有很多种，例如分段机制和分页机制这里我也没有打算写，后面有空再写写分段和分页区别和缺页异常和内存页替换策略，怎么在物理内存不够用的情况下去解决这个问题等...。
   其他  如果对相关内容感兴趣，可以去看看上海交大陈海波教授的操作系统课程，地址：https://ipads.se.sjtu.edu.cn/courses/os/，当然这个部分内存只支持内网访问。</description>
    </item>
    
    <item>
      <title>Go Slice Expansion Mechanism</title>
      <link>http://blog.ibyte.me/post/go-slice-expansion-mechanism/</link>
      <pubDate>Sun, 26 Sep 2021 23:06:08 +0800</pubDate>
      
      <guid>http://blog.ibyte.me/post/go-slice-expansion-mechanism/</guid>
      <description>切片是Golang里面一个复合数据类型，可以把它看做为一个可变长度的数组，和动态数组一样，在创建的时候我们可以指定容量大小，如果不够了，它还能指定扩容，基本的crud没有什么可说的本篇文章将写写切片底层的实现。
   slice struct  其实slice在底层就是一个struct声明一个结构体，结构如下：
1 2 3 4 5  type slice struct { array unsafe.Pointer len int cap int }   slice本身一个结构体里面包含了array和len和cap成员变量，array是一个指针指向真正存储数据的内存头元素地址，len记录着当前实际的元素个数，cap记录当前切片的容量，如上图所示。
   append expansion  在说append扩容机制之前，先看一个下面的题目，最终输出什么？？
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  package main import ( &amp;#34;fmt&amp;#34; ) func main() { array := [...]int{1,2,3,4,5} s1 := append(array[:3],array[4:].</description>
    </item>
    
    <item>
      <title>Computer Science Learning</title>
      <link>http://blog.ibyte.me/post/computer-science-learning/</link>
      <pubDate>Fri, 24 Sep 2021 21:38:31 +0800</pubDate>
      
      <guid>http://blog.ibyte.me/post/computer-science-learning/</guid>
      <description>本页面是本博主个人平时整理的一些计算机科学专业视频学习资料，整理放在下面方便大家浏览，希望对大家有帮助。
   资料名称 链接     数据库基础 哔哩哔哩视频   数据结构基础 哔哩哔哩视频   操作系统基础 上海交通大学-陈海波    </description>
    </item>
    
    <item>
      <title>Often Used Structured Query Languages</title>
      <link>http://blog.ibyte.me/post/often-used-structured-query-languages/</link>
      <pubDate>Tue, 21 Sep 2021 18:52:13 +0800</pubDate>
      
      <guid>http://blog.ibyte.me/post/often-used-structured-query-languages/</guid>
      <description>概 述  在开发过程中，作为一个crud boy来说会使sql来操作数据库增删改查是必不可少的，这篇文章将写写日常开发中常用的sql语句。数据是一个抽象的的定义，所谓的数据库就是把一些元数据通过特定规律整理到一起管理起来，方便通过他特定DDL，DCL，DML这些特定的语句来方便管理数据。
 DDL (data definition languages) 方便开发者通过SQL来定义存储的数据格式，组织数据。 DML (data manipulation languages) 允许用户对数据进行create、delete、insert、updated操作。 DCL (data control languages) 可以来管理用户对数据的访问操作权限，例如检索，更新，删除 &amp;hellip;     查询语句  在日常开发过程中查询数据是应用场景最多的，查询就要使用select关键字，如下：
1  select*fromtableName;  例如有一个表结构如下：
上面是查询整张表的所有字段数据，如果需要筛选就需要制定其field name如下：
1  selectnameas&amp;#34;用户名&amp;#34;fromTableName;  并且上面使用了as关键字并进行了别名，查询出来数据就按照别名进行展示。
如果想查询指定的数据,就要使用where子语句，如下：
1  select*fromTableNamewhereID=1;  那么上的SQL语句查询出来的结果就是上图中ID等于1的一条数据。
如果需要整理结果集去重复的话在前面加入distinct关键字，就可以去重了。
   查询过滤  有时候我们通过条件查询到的数据需要进行其他筛选，这个时候我们就要使用到limit或者order by进行过滤或者重新整理了。
例如我要查询表中年龄最小那个用户的名字，这里不排除年龄是重复的情况，如下：
1  selectusernamefromTableNameorderbyagedesclimit1;  上面就是对 age进行降序然后取一条数据。
1  select*fromTableNameorderbyageASC;  对表进行按照age升序排序整理输出结果。</description>
    </item>
    
    <item>
      <title>Golang Database Reverse Engineering</title>
      <link>http://blog.ibyte.me/post/golang-database-reverse-engineering/</link>
      <pubDate>Thu, 16 Sep 2021 22:49:08 +0800</pubDate>
      
      <guid>http://blog.ibyte.me/post/golang-database-reverse-engineering/</guid>
      <description>相信大家在开发的过程中会去编写一些数据库表对应的model，工作比较重复并且低效，本文将介绍笔者写的一个工具可以根据数据库表生产对应的model逆向工程工具。
   什么是s2s？  s2s (sql to structure)是一款命令行数据库逆向工程工具，它可以通过数据库表生成对应的Java、Go、Rust结构体（class），后面将陆续支持更多的语言。
   配置数据库源  s2s依赖于你的数据库，所以需要你配置好你的数据库连接信息，以便s2s会正常的运行。配置信息方法很简单你只需要在你的环境变量中加入以下信息即可。
推荐使用开发环境的下的root用户登录，因为工具需要information_schema表的权限。
1 2 3 4 5  #s2s 命令的数据库信息 export s2s_host=&amp;#34;127.0.0.1:3306&amp;#34; export s2s_user=&amp;#34;root&amp;#34; export s2s_pwd=&amp;#34;you db password&amp;#34; export s2s_charset=&amp;#34;utf8&amp;#34;   windows的配置此电脑-&amp;gt;属性-&amp;gt;高级系统设置-&amp;gt;环境变量，Mac和Linux则在~/.profile或者~/.zshrc中添加以上配置信息即可。
   使用方法    你可以克隆下载本代码库，然后如果你的电脑上已经安装好了go的编译器那么就进入主目录即可使用go build命令编译生成二进制程序文件。
  如果你觉得麻烦即可在下面列表中找到你对应的平台架构下载对应的二进制可执行文件到电脑上，如果你想在系统上随意调用你则只需要把s2s的安装目录放入你的环境变量中。
  目前对Rust部分数据类型支持不够友好，不过不耽误使用，目前被生成的数据库表名格式必须为user_info这样的snake case这种格式！！后面会考虑修复这个bug。
     平台 地址     Windows-x64 s2s-windows-x64.zip   Mac-x64 s2s-darwin-x64.</description>
    </item>
    
    <item>
      <title>Rust Lifetime</title>
      <link>http://blog.ibyte.me/post/rust-lifetime/</link>
      <pubDate>Thu, 16 Sep 2021 22:47:17 +0800</pubDate>
      
      <guid>http://blog.ibyte.me/post/rust-lifetime/</guid>
      <description>lifetime寿命  Rust中的每一个引用都有一个有效的作用域，生命周期就是为这个作用域服务的，大部分生命周期编译器可以推断出来，可以是隐式的。但是如果在某些情况下编译器就无法正常推断出来了，需要我们自己手动标注，标注生命周期语法就是&#39;a这样的语法。
   为什么需要生命周期？  例如下面例子就是在两个字符串切片里面查找最长的那个并且返回！
1 2 3 4 5 6 7 8  // &amp;#39;a 是指3个引用的作用域生命周期要一致 fn find_long_str&amp;lt;&amp;#39;a&amp;gt;(x: &amp;amp;&amp;#39;a str,y: &amp;amp;&amp;#39;a str)-&amp;gt; &amp;amp;&amp;#39;a str {ifx.len()&amp;gt;y.len(){x}else{y}}  上面我就加注了生命周期标识符，如果不加编译器会报错，原因是因为我们这个函数引用的是外部的变量，不能确定引用的变量是否已经被销毁了，那这样就是悬垂引用！
1 2 3 4 5 6 7 8 9 10  letstr1=String::from(&amp;#34;Hello&amp;#34;);letstr2;letresult;{//let str2 = String::from(&amp;#34; World!&amp;#34;); str2=String::from(&amp;#34; World!&amp;#34;);result=find_long_str(str1.as_str(),&amp;amp;str2);}// 这里借用检测就提示 引用了已经销毁的资源了 println!(&amp;#34;{}&amp;#34;,result);  加了生命周期标识符之后，如果我把let str2 = String::from(&amp;quot; World!&amp;quot;);取消注释放在一个内部作用域里面定义，那么这时调用 find_long_str编译器就会报错，因为我在下面出了作用域还使用了find_long_str返回的结果，而这个结果可能就是str2的内容， 使用这个是违反了所有权规则的，str2离开内部作用域就被销毁了。
在标注生命周期fn find_long_str&amp;lt;&#39;a&amp;gt;(x: &amp;amp;&#39;a str, y: &amp;amp;&#39;a str) -&amp;gt; &amp;amp;&#39;a str之后编译器就知道输入参数和返回参数生命周期是要一致的，并且返回值生命周期肯定是取生命周期最短的那个的。</description>
    </item>
    
    <item>
      <title>Rust Box Pointer</title>
      <link>http://blog.ibyte.me/post/rust-box-pointer/</link>
      <pubDate>Thu, 16 Sep 2021 22:46:21 +0800</pubDate>
      
      <guid>http://blog.ibyte.me/post/rust-box-pointer/</guid>
      <description>什么是智能指针？  在Rust中智能指针有很多种，用大白话说就是一个数据结构，实现了一些特殊的trait从而达到某种特性和功能，然后去管理某块内存上的数据，相当于一个盒子一样包装一层，这篇文章将介绍一下Box。
   Deref  Deref这个trait只要实现了它那么你自定义的结构体，就可以使用*解引用拿取你结构体里面所包装的数据，熟悉Rust都知道*是用来解引用的，下面的I32Box(i32)就是实现了Deref从而达到直接使用*拿取出内部的数据。
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  usestd::ops::Deref;struct I32Box(i32);// impl Deref implDerefforI32Box{type Target=i32;fn deref(&amp;amp;self)-&amp;gt; &amp;amp;Self::Target{&amp;amp;self.0}}implI32Box{fn new(v:i32)-&amp;gt; Self{I32Box(v)}}fn main(){letn=I32Box::new(1024_i32);println!(&amp;#34;{}&amp;#34;,*n);}     Drop  Rust里面有drop函数，也有一个Drop的trait同样如果自定义的结构体实现这个Drop，那么就你变量离开作用域的时候执行自定义Drop的方法的逻辑。
1 2 3 4 5  implDropforI32Box{fn drop(&amp;amp;mutself){println!(&amp;#34;啊，挂了，I32Box({:?})被清理了！&amp;#34;,self.0)}}  例如为I32Box实现Drop的drop函数，运行：
1 2 3 4 5 6 7  Compiling playground v0.</description>
    </item>
    
    <item>
      <title>Traffic Restrictions</title>
      <link>http://blog.ibyte.me/post/traffic-restrictions/</link>
      <pubDate>Thu, 16 Sep 2021 22:45:00 +0800</pubDate>
      
      <guid>http://blog.ibyte.me/post/traffic-restrictions/</guid>
      <description>前 言   在开发高并发系统时，我们可能会遇到接口访问频次过高，为了保证系统的高可用和稳定性，这时候就需要做流量限制，你可能是用的 Nginx 这种 Web Server 来控制请求，也可能是用了一些流行的类库实现。限流是高并发系统的一大杀器，在设计限流算法之前我们先来了解一下它们是什么。
    限 流  限流的目的是通过对并发访问请求进行限速，或者对一个时间窗口内的请求进行限速来保护系统，一旦达到限制速率则可以拒绝服务、排队或等待、降级等处理。通过对并发（或者一定时间窗口内）请求进行限速来保护系统，一旦达到限制速率则拒绝服务（定向到错误页或告知资源没有了）、排队等待（比如秒杀、评论、下单）、降级（返回兜底数据或默认数据）。
如 图:
如图上的漫画，在某个时间段流量上来了，服务的接口访问频率可能会非常快，如果我们没有对接口访问频次做限制可能会导致服务器无法承受过高的压力挂掉，这时候也可能会产生数据丢失，所以就要对其进行限流处理。
限流算法就可以帮助我们去控制每个接口或程序的函数被调用频率，它有点儿像保险丝，防止系统因为超过访问频率或并发量而引起瘫痪。我们可能在调用某些第三方的接口的时候会看到类似这样的响应头：
1 2 3  X-RateLimit-Limit:60//每秒60次请求X-RateLimit-Remaining:22//当前还剩下多少次X-RateLimit-Reset:1612184024//限制重置时间  上面的 HTTP Response 是通过响应头告诉调用方服务端的限流频次是怎样的，保证后端的接口访问上限。为了解决限流问题出现了很多的算法，它们都有不同的用途，通常的策略就是拒绝超出的请求，或者让超出的请求排队等待。
一般来说，限流的常用处理手段有：
 计数器 滑动窗口 漏桶 令牌桶     计数器   计数器是一种最简单限流算法，其原理就是：在一段时间间隔内，对请求进行计数，与阀值进行比较判断是否需要限流，一旦到了时间临界点，将计数器清零。 这个就像你去坐车一样，车厢规定了多少个位置，满了就不让上车了，不然就是超载了，被交警叔叔抓到了就要罚款的，如果我们的系统那就不是罚款的事情了，可能直接崩掉了。
  可以在程序中设置一个变量 count，当过来一个请求我就将这个数 +1，同时记录请求时间。 当下一个请求来的时候判断 count 的计数值是否超过设定的频次，以及当前请求的时间和第一次请求时间是否在 1 分钟内。 如果在 1 分钟内并且超过设定的频次则证明请求过多，后面的请求就拒绝掉。 如果该请求与第一个请求的间隔时间大于计数周期，且 count 值还在限流范围内，就重置 count。  代码实现:
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66  package main import ( &amp;#34;log&amp;#34; &amp;#34;sync&amp;#34; &amp;#34;time&amp;#34; ) type Counter struct { rate int //计数周期内最多允许的请求数  begin time.</description>
    </item>
    
    <item>
      <title>Web Safe Csrf</title>
      <link>http://blog.ibyte.me/post/web-safe-csrf/</link>
      <pubDate>Thu, 16 Sep 2021 22:43:44 +0800</pubDate>
      
      <guid>http://blog.ibyte.me/post/web-safe-csrf/</guid>
      <description>概 述  随着互联网的高速发展，信息安全问题已经成为企业最为关注的焦点之一，而前端又是引发企业安全问题的高危据点。在移动互联网时代，前端人员除了传统的 XSS、CSRF 等安全问题之外，又时常遭遇网络劫持、非法调用 API 等新型安全问题。当然，浏览器自身也在不断在进化和发展，不断引入 CSP、Same-Site Cookies 等新技术来增强安全性，但是仍存在很多潜在的威胁，这需要我们技术人员不断对系统进行“查漏补缺”。
   废话少说，先看问题  为了模拟问题我这边用go写了2个服务端的代码，正常交易系统的API，当然这里只是为了演示漏洞利用，代码比较简单如下:
transaction.go 内容:
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86  package main import ( &amp;#34;errors&amp;#34; &amp;#34;fmt&amp;#34; &amp;#34;github.</description>
    </item>
    
    <item>
      <title>Golang Web Session Implement</title>
      <link>http://blog.ibyte.me/post/golang-web-session-implement/</link>
      <pubDate>Thu, 16 Sep 2021 22:41:38 +0800</pubDate>
      
      <guid>http://blog.ibyte.me/post/golang-web-session-implement/</guid>
      <description>概 述  大家都知道 session 是web应用在服务器端实现的一种用户和服务器之间认证的解决方案，目前 Go 标准包没有为 session 提供任何支持，本文我将讲解session的实现原理，和一些常见基于session安全产生的防御问题。
当然有人可能看了会抬杠，说现在大部分不是前后端分离架构吗？对，你可以使用JWT解决你的问题。但是也有一些一体化web应用需要session，所以我准备造个轮子。自己造的轮子哪里出问题了，比别人更熟悉，有bug了，还不用求着别人修bug,自己修就好了，呵呵哈哈哈，当然这几句话有点皮😜。
   需 求  我觉得一名好的程序员，在写程序之前应该列一下需求分析，整理一下思路，然后再去写代码。
 支持内存存储会话数据 支持分布式redis会话存储 会话如果有心跳就自动续命30分钟（生命周期） 提供防御：中间人，会话劫持，会话重放等攻击     工作原理  首先必须了解工作原理才能写代码，这里我就稍微说一下，session是基于cookie实现的，一个session对应一个uuid也是sessionid，在服务器创建一个相关的数据结构，然后把这个sessionid通过cookie让浏览器保存着，下次浏览器请求过来了就会有sessionid，然后通过sessionid获取这个会话的数据。
   代码实现  都是说着容易，实际写起来就是各种坑，不过我还是实现了。
少说废话，还是直接干代码吧。
 依赖关系  上面是设计的相关依赖关系图，session是一个独立的结构体， GlobalManager是整体的会话管理器负责数据持久化，过期会话垃圾回收工作♻️，storage是存储器接口，因为我们要实现两种方式存储会话数据或者以后要增加其他持久化存储，所以必须需要接口抽象支持，memory和redis是存储的具体实现。
storage接口  1 2 3 4 5 6 7 8 9  package sessionx // session storage interface type storage interface { Read(s *Session) error Create(s *Session) error Update(s *Session) error Remove(s *Session) error }   storage就9行代码，是具体的会话数据操作动作的抽象，全部参数使用的是session这个结构的指针，如果处理异常了就即错即返回。</description>
    </item>
    
    <item>
      <title>Redis Distributed Lock</title>
      <link>http://blog.ibyte.me/post/redis-distributed-lock/</link>
      <pubDate>Thu, 16 Sep 2021 22:37:49 +0800</pubDate>
      
      <guid>http://blog.ibyte.me/post/redis-distributed-lock/</guid>
      <description>前 言  谈到分布式应用那必然离不开分布式锁🔐的问题，分布式锁在分布式应用中应用广泛，本文就讲讲基于redis实现的分布式锁的一些问题。
   锁  可能各位coder接触最多的还是在多线程的环境下，为了保证一个代码块在同一时间只能由一个线程访问情况，例如下图：
对于单进程的并发场景，可以使用编程语言及相应的类库提供的锁，如Java中的 synchronized 语法以及 ReentrantLock，Golang中的sync包下面的mutex，Rust中的async_std::sync::Mutex，避免并发问题，这实际上是本地锁的方式。
   分布式锁  但是现在流行的分布式架构，在分布式环境下，如何保证不同节点的线程同步执行呢？或者共享资源怎么上锁呢？？？
在将应用拆分为分布式应用之前的单机系统中，对一些并发场景读取公共资源时如扣库存，卖车票之类的需求可以简单的使用同步或者是加锁就可以实现，但是应用分布式了之后系统由以前的单进程多线程的程序变为了多进程多线程，这时使用以上的解决方案明显就不够了。
一般业界有几种解决方:
 基于 DB 的唯一索引 基于 Memcached的 add 命令 基于 Zookeeper 的临时有序节点 基于 Redis 的NX EX 基于Chubby粗粒度分布式锁服务     Redis的坑你填了几个？   如果在分布式场景中，实现不同客户端的线程对代码和资源的同步访问，保证在多线程下处理共享数据的安全性，就需要用到分布式锁技术，我就来写写基于Redis的一些坑😁。
 在分布式时，在程序中修改已有数据时，需要先读取，然后进行修改保存，此时很容易遇到并发问题。由于修改和保存不是原子操作，在并发场景下，部分对数据的操作可能会丢失，本地锁无法在多个服务器之间生效，这时候保证数据的一致性就需要分布式锁来实现。
Redis 锁主要利用 Redis 的 setnx 命令实现，
 加锁命令：SETNX key value，当键不存在时，对键进行设置操作并返回成功，否则返回失败,KEY 是锁的唯一标识，一般按业务来决定命名。 解锁命令：DEL key，通过删除键值对释放锁，以便其他线程可以通过 SETNX 命令来获取锁。 锁超时：EXPIRE key timeout, 设置 key 的超时时间，以保证即使锁没有被显式释放，锁也可以在一定时间后自动释放，避免资源被永远锁住。  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  // 伪代码实现 fn main(){letkey: &amp;amp;&amp;#39;static str =&amp;#34;sync_lock&amp;#34;;ifup_lock(key,1)==1{// 设置超时 expire(key,30)// .</description>
    </item>
    
    <item>
      <title>Alibaba Hot Ring Perceive</title>
      <link>http://blog.ibyte.me/post/alibaba-hot-ring/</link>
      <pubDate>Thu, 16 Sep 2021 22:27:07 +0800</pubDate>
      
      <guid>http://blog.ibyte.me/post/alibaba-hot-ring/</guid>
      <description>👨‍💻‍：直奔话题 HotRing，少说废话。   去年阿里的Tair团队发表过一篇论文HotRing: A Hotspot-Aware In-Memory Key-Value Store，这篇论文里面讲述了阿里的最快的KV分布式存储引擎核心HotRing的技术与实现，我看了他们的数据显示其引擎吞吐性能可达600M ops/s，这个速度比传统的KVS系统2.58倍的性能提升，于是我就抱着好奇去看看了那篇论文然后就有这篇文章。
    👨‍💻‍： 有问题就有故事？  一个新东西或者技术出现，那就说明老的那套方案技术存在某种缺陷或者满足不了某种的特殊需求了&amp;hellip;&amp;hellip;
问题动机就是：某一时刻流量上来了，系统扛不住了&amp;hellip; 只要说到高并发，相信这个回答占大多数。HotRing的出现也是这个原因，真的是面向问题编程😜。。。挖槽，真的验证这句话了。。。
这张图就是传统的kvs架构图，不管你中间的那一层用什么，在某些时刻，缓存系统迎来巨大的访问量（双11秒杀），可能存在访问倾斜(什么？你问我什么是访问倾斜，好吧你不适合编程。😜)，大多数访问集中在极少数数据上（例如微博热点事件）。
对于集群级别的热点监测倍受重视，如一致性 Hash，某个节点上的数据被读写访问次数远远高出其他节点，对此类行为没有很好的监测到。
还有单机热问题上没有优化，例如计算机内存的KV存储热点和查询速度没有优化。
一个好的存储这些事情都是必须去做的，从上到下，从下到上，每层都在解决一部分问题，总和就是一个大的问题，如果你不能把问题拆成小问题，那你那个问题不一定能解决的好，例如CPU Cache、LevelDB的设计。
   👨‍💻‍：HotRing 干了什么？？？  现有技术，内存 KVS 对时延要求是很高的，一个在并发中无锁的数据结构尤为重要，然后就是一个能让热点数据能自我跑到最容易访问的地方提高读取速度的。
 热点是动态变化的，如何检测，如何转移?  这个问题是逃不开的，在HotRing的设计，论文中提到了一个概念就是有序环。
它做了什么？看得懂上图就能看出来，他把传统的Map的底层的存储链表+数组结构换成了一个环形链结构，然后通过head自由的控制指向哪一个节点。
传统的结构可能你的数据在最后一层或者数据非常多，程序只能通过head顺序查找，时间复杂度O(n)
而环形的结构，避免多次遍历希望把热点数据放在冲突链前面，传统的则需要不断修改节点，需要不断移动节点，必须从头节点开始，到尾节点终止。移动头指针的情况下，会导致一些节点无法被访问。
移动热点不方便，需要把中间节点移动到头节点去，移动链表中的节点很复杂且难以做到无锁并发，就上图这个操作就要3步。
改成环链，Head可以指向任意节点，即从任意节点开始遍历，然后还能对数据进行排序。
为什么有序???  这个就属于环链的特点了，没有终结点，如果查找值不存在，无法判断何时终结。改成有序的了，可以根据前后项的关系判断是否终结本次查询。
 前驱节点 &amp;lt; 待查找节点 &amp;lt;后驱节点 miss 前驱节点 &amp;gt; 后驱节点 &amp;amp;&amp;amp; 待查找节点 &amp;lt; 后驱节点 miss 前驱节点 &amp;gt; 后驱节点 &amp;amp;&amp;amp; 待查找节点 &amp;gt; 前驱节点 miss 待查找节点 == 节点 K hit  排序的过程就是利用key排序可以解决这个问题，若目标key介于连续两个item的key之间，说明为read miss操作，即可终止返回。由于实际系统中，数据key的大小通常为10~100B，比较会带来巨大的开销。哈希结构利用tag来减少key的比较开销。</description>
    </item>
    
    <item>
      <title>Compound Data Type of Rust</title>
      <link>http://blog.ibyte.me/post/compound-data-type-of-rust/</link>
      <pubDate>Thu, 16 Sep 2021 22:25:30 +0800</pubDate>
      
      <guid>http://blog.ibyte.me/post/compound-data-type-of-rust/</guid>
      <description>概 述   好久没有更新rust相关的内容了，更新一波Rust的内容，本篇讲介绍一下Rust中的复合数据类型。
    Composite Type  复合数据类型是一种数据类型，它可以原始的基本数据类型和其它的复合类型所构成， 构成一个复合类型的动作，又称作组合。
本文讲介绍一下在Rust中有tuple、array、struct、enum几个复合类型。
   tuple  tuple即元组，元组类型是由多个不同类型的元素组成的复合类型，通过()小括号把元素组织在一起成一个新的数据类型。元组的长度在定义的时候就已经是固定的了，不能修改，如果指定了元素的数据类型，那么你的元素就要对号入座！！！否则编译器会教训你！
例子：
1 2 3 4 5 6 7 8 9 10 11  fn main(){// 指定数据类型 lettup_type:(i8,i32,bool)=(21,-1024,true);// 解构元素 let(one,two,three)=tup_type;// 二维的元组 lettup_2d:(f64,(i8,i32,bool))=(3.1415927,(one,two,three));println!(&amp;#34;tup_2d = {:?}&amp;#34;,tup_2d);// 索引 println!(&amp;#34;π = {:?}&amp;#34;,tup_2d.0);}  元组的访问方式有好几种，通过下标去访问，也可以使用解构赋值给新的变量去访问，但是不支持迭代器去访问。
1 2 3  forvintup_2d.1.iter(){println!(&amp;#34;{}&amp;#34;,v)}  1 2 3 4 5 6 7 8 9 10 11 12 13  Compiling playground v0.</description>
    </item>
    
    <item>
      <title>Rust Falsework Desgin</title>
      <link>http://blog.ibyte.me/post/rust-falsework-desgin/</link>
      <pubDate>Thu, 16 Sep 2021 22:22:45 +0800</pubDate>
      
      <guid>http://blog.ibyte.me/post/rust-falsework-desgin/</guid>
      <description>前言   断断续续看了半个月的Rust书，学了点基础，不知道做什么好？？于是我想着用业余时间撸一个command line application的骨架。然后就有了https://github.com/auula/falsework这个项目，falsework可以帮助你快速构建一个命令行应用。这篇文章写写falsework的使用并在后部分介绍一下怎么实现的。
 看了看这张图，那我估计还在坡道起步。。
   导入依赖   https://crates.io/crates/falsework  在你的项目中添加依赖如下：
1 2  [dependencies] falsework = &amp;#34;0.1.3&amp;#34;      快速构建  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45  usestd::error::Error;usefalsework::{app,cmd};fn main(){// 通过falsework创建一个骨架 letmutapp=falsework::app::new();// 应用元数据信息 app.</description>
    </item>
    
    <item>
      <title>Golang Channel Desgin</title>
      <link>http://blog.ibyte.me/post/golang-channel-desgin/</link>
      <pubDate>Thu, 16 Sep 2021 22:19:16 +0800</pubDate>
      
      <guid>http://blog.ibyte.me/post/golang-channel-desgin/</guid>
      <description>相信大家在开发的过程中经常会使用到go中并发利器channel，channel 是CSP并发模型中最重要的一个组件，两个独立的并发实体通过共享的通讯channel进行通信。大多数人只是会用这么个结构很少有人讨论它底层实现，这篇文章讲写写channel的底层实现。
   channel  channel的底层实现是一个结构体，源代码如下:
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  type hchan struct { qcount uint // total data in the queue  dataqsiz uint // size of the circular queue  buf unsafe.Pointer // points to an array of dataqsiz elements  elemsize uint16 closed uint32 elemtype *_type // element type  sendx uint // send index  recvx uint // receive index  recvq waitq // list of recv waiters  sendq waitq // list of send waiters  // lock protects all fields in hchan, as well as several  // fields in sudogs blocked on this channel.</description>
    </item>
    
    <item>
      <title>Golang Memory Escape Analysis</title>
      <link>http://blog.ibyte.me/post/golang-memory-escape-analysis/</link>
      <pubDate>Thu, 16 Sep 2021 22:07:09 +0800</pubDate>
      
      <guid>http://blog.ibyte.me/post/golang-memory-escape-analysis/</guid>
      <description>前 言  很多时候为了更快的开发效率，大多数程序员都是在使用抽象层级更高的技术，包括语言，框架，设计模式等。所以导致很多程序员包括我自己在内对于底层和基础的知识都会有些生疏和，但是正是这些底层的东西构建了我们熟知的解决方案，同时决定了一个技术人员的上限。
在写C和C++的时候动态分配内存是让程序员自己手动管理，这样做的好处是，需要申请多少内存空间可以很好的掌握怎么分配，但是如果忘记释放内存，则会导致内存泄漏。
Rust又比👆上面俩门语言分配内存方式显得不同，Rust的内存管理主要特色可以看做是编译器帮你在适当的地方插入delete来释放内存，这样一来你不需要显式指定释放，runtime也不需要任何GC，但是要做到这点，编译器需要能分析出在什么地方delete，这就需要你代码按照其规则来写了。
相比上面几种的内存管理方式的语言，像Java和Golang在语言设计的时候就加入了garbage collection也就runtime中的gc，让程序员不需要自己管理内存，真正解放了程序员的双手，让我们可以专注于编码。
   函数栈帧  当一个函数在运行时，需要为它在堆栈中创建一个栈帧（stack frame）用来记录运行时产生的相关信息，因此每个函数在执行前都会创建一个栈帧，在它返回时会销毁该栈帧。
通常用一个叫做栈基址（bp）的寄存器来保存正在运行函数栈帧的开始地址，由于栈指针（sp）始终保存的是栈顶的地址，所以栈指针保存的也就是正在运行函数栈帧的结束地址。
销毁时先把栈指针（sp）移动到此时栈基址（bp）的位置，此时栈指针和栈基址都指向同样的位置。
   Go内存逃逸  可以简单得理解成一次函数调用内部申请到的内存，它们会随着函数的返回把内存还给系统。下面来看看一个例子：
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  package main import &amp;#34;fmt&amp;#34; func main() { f := foo(&amp;#34;Ding&amp;#34;) fmt.Println(f) } type bar struct { s string } func foo(s string) bar { f := new(bar) // 这里的new(bar)会不会发生逃逸？？？  defer func() { f = nil }() f.</description>
    </item>
    
  </channel>
</rss>

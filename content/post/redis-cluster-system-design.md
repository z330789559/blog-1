---
title: "Redis Cluster System Design"
date: 2021-11-24T14:37:48+08:00
---

## 为什么会出现集群架构？

上一篇写到了`redis`哨兵模式，这种模式虽然能解决一些并发情况下的数据请求压力问题和可用性问题，主从替换以及故障恢复。哨兵模式只是`redis`架构在历史演变过程中一个的转折点，哨兵这种模式主从模式的数据都是全量同步的，也就是每个从节点上的数据都是主节点上的数据副本，大家想一想有木有问题？

是不是太浪费内存了？或者说可不可以把全量的数据拆解成部分数据，然后分布在不同节点上？那么这种有什么好处吗？？？

如果数据全部存储在一台机子上，全量同步的时候会时间过长，有时候出现了`big key`情况更是一个问题，网络一波动，本来传输正在进行中，一半断掉了，会增加同步失败的概率。分片的存储好处，是不是可以减低这些风险的，每个节点我存储一部分数据，实质我们可以交叉存储数据块，`a`节点存储`b`节点一个副本，`b`节点存储`a`节点一个副本，这样就算如果`a`当掉了，我们是不是可以去`b`里面拿取副本继续使用，当然这个我是自己的一些看法，我看`hdfs`就是这么设计的，当然`redis`没有这个交叉存储策略，但是笔者认为如果在`key`下功夫也可以达到的，把`key`以特定格式的命名的格式二次存储也能达到，只是说浪费一点内存吧了。

## 存储模型

有木有想过为什么`redis`为什么是`k:v`这种存储模型，而不是直接设计成关系型数据库那样的？？熟悉`redis`应该都了解一点历史，使用`key-value`是有历史原因的，早期的时候，`redis`作者起初是为提高自己网站的读写并发能力而创建`redis`，而`key-value`字典结构的常规时间复杂度是`O(1)`，刚好满足作者的性能要求和业务数据结构需求。

`redis`大部分作用都是缓存数据的，大部分数据都是从`db`读取的，而且这些数据也不是`db`中全量的数据，只是一部分而已，没有高度组织化结构化数据，用一部分数据，怎么组成一个完整的关系模型？所以不需要结构化查询语言。

## Codis分片方案

正是`redis`作者这些历史原因，本来在设计之初可能也就是单机就可以满足作者自己的需求，但是没想到它写的这个软件被这么多人使用，影响力这么大，然后出现了主从模式，哨兵模式，直到本文要讲的集群模式。

> 计算机科学领域的任何问题都可以通过增加一个间接的中间层来解决!

`Redis`的集群也有演变历史，早期有`Twemproxy`是`Twitter`团队的开源的解决分片方案，还有`Codis`的方案值得注意这是一个国人开源的，也就是现在`pingcap`几位创始人，前豌豆荚团队的，不得不说`屌爆了`。

他们方案都有一个共同特点，都是通过一个`proxy`代理层进行的分片处理的，不知道有没有使用过`Nginx`的，使用过大家应该知道`Nginx`反向代理一些服务程序，例如`Nginx`和`Tomcat`组成一个反向代理负载均衡的集群，`codis`也是类似这样的方案。

![codis代理方案](https://tva1.sinaimg.cn/large/008i3skNgy1gwob6q2qnnj30ih071q2y.jpg)

`Codis`就是起着一个中间代理的作用，`Codis`内部维护一个`key`映射算法，客户端访问`codis`和直接访问`redis`没有区别的，因为`codis`实现了`redis`的通讯协议，`协议`这个东西很重要的，双方都商量好的怎么处理数据，做起事来，就很方便，大家懂得。。。。`Codis`是一个无状态的，所以可以增加多个`Codis`来提升`QPS`,同时也可以起着扩容的作用。

**`Codis`怎么工作的？**

1. 在`Codis`会把所有的`key`分成`1024`个槽，这`1024`个槽对应着的就是`Redis`的集群，这个在`Codis`中是会在内存中维护着这`1024`个槽与`Redis`实例的映射的，当然这个是可以配置，就看你的`Redis`的节点数量有多少，偏多的话，可以设置槽多一些。

2. `codis`会先是把`key`进行`CRC32` 后，得到一个`32`位的数字，然后再`hash%1024`后得到一个余数，这个值就是这个`key`对应着的槽，这槽后面对应着的就是`redis`的实例。

**那么问题来了？我多个`codis`怎么解决槽位共享？**

`Codis`把这个工作交给了`ZooKeeper`来管理，`Codis`节点会监听到`ZooKeeper`的槽位变化，会及时同步过来，如下图

![槽位共享](https://tva1.sinaimg.cn/large/008i3skNgy1gwobtmal3aj30dh0agjrr.jpg)

**这种方式有木有什么弊端？？？**

如果是使用`mset`批量设置值，值是是不是被分散在各个节点上，还有如果`rename`的时候也是一个问题，不过我认为有一种解决方案在映射的时候给`key`生成一个唯一的`uuid`，我`rename`的只是`key`，而映射的时候使用`uuid`计算槽位，当然这些问题在后面的`redis`官方解决方案里面得到了解决。

## 官方集群方案

说起官方集群解决方案，是`redis`作者在前面那些代理中间件出现之后发布的，我看他这个设计之后感觉应该借鉴了`codis`的设计方案，只是把集群槽位信息同步改成`p2p`然后通过`gossip`协议来解决了同步的问题，个人感觉都是都是`redis`在设计之初留下技术负债问题，早期的时候估计作者自己也没有打算写集群方案，被逼的，当然这个是我个人的想法和看法。

相对于 `Codis` 的不同，它是去中心化的，如图下面我画的图，该集群有4个 `Redis` 节点组成， 每个节点负责整个集群的一部分数据，每个节点负责的数据多少可能不一样，这4个节点相 互连接组成一个对等的集群，它们之间通过一种`gossip`协议相互交互集群信息。

![示意图](https://tva1.sinaimg.cn/large/008i3skNgy1gwph0tv6fcj30fm0d474u.jpg)

当然我上面画的图太接近`一致性哈希环`了，但是设计思想是接近的，`Redis Cluster` 将所有数据划分为 `16384` 的 `slots`，比 `Codis` 的 `1024` 个槽划分得更为精细，每个节点负责其中一部分槽，槽位的信息存储于每个节点中，它不像 `Codis`存储在`zookeeper`中，相信大家都听说过`p2p`，不清楚自己去查吧。。

**那么问题来了！客户端怎么获取不同节点中的`key`呢？**

有木有想过，我客户端连接的是`a`节点，而我要的数据在`b`节点上，这个问题怎么解决的呢？`redis`在通讯协议上下了功夫！它借鉴了`http`协议的重定向的概念，大家的应该熟悉`http`中重定向协议吧，如下图：

![重定向过程](https://tva1.sinaimg.cn/large/008i3skNgy1gwphe8i7haj309908ndfz.jpg)

客户端来连接集群时，它也会得到一份集群的槽位配置信息。这样当客户端要查找某个 `key` 时，可以直接定位到目标节点，这么一来就解决了数据重定向拿取问题，上面我说到了`codis的rename`问题，官方解决方案是可以让客户端把每个`key`设置一个`tag`然后指定到对应节点槽位上，想深入了解的自己看看一些资料吧，我这里主要讲大问题思路。

**这种模式就十全十美了吗？没有其他问题了吗？**

从上面我画的图就能看出来，集群内置了`16384`个`slot`，并且把所有的物理节点映射到了这`16384`个`slot`上，或者说把这些`slot`均等的分配给了各个节点，但是这种分片模式，肯定是有问题的，例如一个节点挂掉了，那么整个集群中的一部分数据丢失了，数据就不是完整的了，有什么办法解决？

解决办法还是有的每个分片节点备几个从节点，从主节点上读取数据到从节点上，和哨兵模式一样，有问题自己从节点顶上。

## 小结

`redis`集群采用`P2P`模式，是完全去中心化的，不存在中心节点或者代理节点，都是历史遇到问题就出解决方案演进过来的。估计作者早期也就是打算设计的时候一个单机就能解决问题了，没想到，万万没想到啊，用的人那么多，然后出现分布式问题哈哈哈哈。。所以出现了各种解决方案，不管什么解决方案，`redis`数据不能做到保证强一致性，一些已经向客户端确认写成功的操作，会在某些不确定的情况下丢失，而且由于集群方案都是把一大块数据使用`分治法`打散在各个节点上，支持`acid`更是一个问题，这估计也是在历史设计的时候留下的问题，但是大部分场景`redis`都是做缓存服务器用用可以了。

好了本文是`redis`系统设计系列最后一章了，后面再找找其他`NB`的系统设计写写，好看不过瘾，那你倒是点个关注啊。。。。

## 文末立个flag

`KVBase`要开始动工了，先把`flag`立在这儿，`logo`都设计好了，管它三七二十一把第一个版本撸出来再说，然后慢慢迭代。

![LOGO预览哈哈哈](https://tva1.sinaimg.cn/large/008i3skNgy1gwpi1bqrbhj31hc0dwwfn.jpg)


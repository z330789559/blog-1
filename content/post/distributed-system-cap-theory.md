---
title: "Distributed System Cap Theory"
date: 2022-01-26T23:26:59+08:00
---
## 前言
在分布式系统中大部分应用程序的部署方式是分散的，系统有多个节点分散在个区域，相互之间通过网络进行通讯。一个计算请求可能需要经过集群中的多个节点，然后再返回给客户端，但是影响这个请求的可能是网络速度，部分节点宕机，无响应，响应过慢，来至脏数据交叉污染，这些问题就给分布式系统设计者带来许多挑战，例如：`故障恢复`和`故障提取发现`，`节点存活检测`等等...

![distributed system](https://tva1.sinaimg.cn/large/008i3skNgy1gx4gtovtv9j308c08c0st.jpg)

本人最近一个多月也在研究相关领域的论文资料，这么看下来，我总结几句话：`该出问题的肯定会出问题，我们要做的就是把出错率降低，只要是个程序肯定就会有问题出现，毕竟程序这个东西是跑在计算机上的，数据也是动态的未知的，分布式更是依赖于网络，我们要做的就是在设计的时候考虑未来系统或者在出错或者出问题的时候怎么解决，把出错率降低，提高可用性。`

## 什么原因会引发故障？

首先需要明确的是，只有当单个节点的处理能力无法满足日益增长的计算、存储任务的时候，且硬件的提升（加内存、加磁盘、使用更好的`CPU`）高昂到得不偿失的时候，应用程序也不能进一步优化的时候，我们才需要考虑分布式系统。分布式系统容错性，个人认为是分布式系统设计中最重要的部分，打个比方，一个集群有一个节点出现问题，不可能说导致我整个系统都不能使用了。所以后面出现了一些分布式故障检测机制，可以根据故障类型来给出解决方案，然后出现了分布式数据一致性算法。

分布式系统中的一些数据一致性问题可以看成平时在写单机程序出现的并发和并行资源竞争问题一样，不知道大家有木有写过`go`语言，`go语言`在处理数据竞争或者说并行任务通讯的时候，采用了`CSP`来共享状态，也就是通过消息来共享程序里面的线程或者协程状态的设计，用消息来互相通讯，这是一个很棒👍🏻的设计。

在分布式系统设计中也是这样的，一个集群里面的节点通过网络互相来通讯，可是用网络怎么做到不丢消息？网络数据包顺序问题？响应速度问题，一个请求进来了，但是这个请求处理速度要依赖于集群中处理计算最慢那个节点的速度，然后在返回，这么一折腾，分布式系统坑太多了。。这里还没有说到分布式数据一致性问题，还是单单基于网络通信出现的问题和各个节点本身的问题。。。

**1. 请求处理**

大部分应用都是接到客户端发送的请求，然后处理请求，如果是简单的计算请求，可能就几毫秒就处理完成了，但是一个需要消耗时间的请求就需要耗费大量时间处理，一个请求还好，如果是大量的耗时请求，那么再小的问题，也随着请求流量增多，而加大。此时即使分布式系统，最终处理也是在每个计算节点上完成的，还是会落到当个节点上，然后汇总返回结果给客户端。

大家都知道生产者与消费者模型，在二者中间加一个缓冲区，来缓解当前处理系统压力，在分布式系统上，也采用这个设计，根据生产者生成数据速度，动态调整缓存队列大小，把请求处理管道化，也可以吸收一瞬间产生的巨大流量，也使得接收和处理在时间上分开。

**2. 时间和数据包顺序**

集群中的节点都是分开的，时间这个也会影响计算，可能`A`节点向`B`节点发送了`2`个数据包，但是这`2`数据包，达到`B`节点时发现顺序反了，换句话说就是这两个数据包需要按照前后顺序处理，可以在数据包里标记序号解决。

一些强依赖于时间的程序，就对时间问题有很高要求，可以抽出一个统一的时间签发领导节点，负责整个集群的时间管理工作，当然这是看完相关论文笔记，如果读者对这些感兴趣，你们直接研究吧，这块确实水深啊。

**3. 级联和关联故障**

相信大家如果做个微服务开发，或者对微服务有了解的应该知道熔断机制，就和电流里面的保险丝一样，起到保护作用。`关联故障`分布式系统逃不开的话题，由于系统是集群模式，一个节点出现的故障，可能就会联系到与它关联的节点上，从一个系统传播到另外一个系统，如果不做处理，最后可能就会污染整个分布式系统来至崩溃。当一台服务器无响应的时候，可能一个客户端会无数次的循环发出请求，可以设置请求重试时间窗口来消处抖动。


## 故障监测
虽然单个节点的故障概率较低，但节点数目达到一定规模，出故障的概率就变高了，分布式系统需要保证故障发生的时候，系统仍然是可用的，这就需要监控节点的状态，在节点故障的情况下将该节点负责的计算、存储任务转移到其他节点上继续处理操作。

为了系统更加可用，分布式系统引入故障检测是必须的，在程序运行的时候我们开发者是根本不知道什么时候会出错，运行时候的程序里面的数据都是动态的。`故障检测器`几乎是每个分布式系统必不可缺的组件，能帮助整个系统去分辨其他节点是挂了还是卡住了，还是崩溃了。。。。由于是分布式系统靠着网络通讯，所以最常见的办法就是通过`心跳包`来解决，但是我不知道大家有没有了解过网络传输协议，`tcp`和`udp`，或者大家在开发的时候，两个独立运行的系统为什么要用去`kafka`去传递消息，而不是自己用`tcp`去写一个服务器和客户端互相传递消息？网络这个东西本来就是不可靠的，我发送一个数据包，然后这个数据包丢了，或者举个比较奇葩的例子：一个盗墓的把你两个机房之间的光缆挖断了。。。都是不可靠的。。

说到了`kafka`，正好写一下，`kafka`消息也是基于网络传输，只是他作为一个消息中间件，在原有的网络协议上，增加了一些特殊的策略去保证消息在网络上传递是可靠的。

言归正传，故障检测这个在分布式系统也是一个大坑，常见也就是一个心跳包去检查节点，默认可能节点设置的有心跳包生命周期的，一个心跳包发送出去了，可能我需要`3`秒内得到回复，但是巧好网络问题，这个心跳包是`5`秒钟回复的，此时其他节点生命周期过了，认为这个节点可能挂了，但是其实又没有挂。。。然后有出现了一种无超时的心跳检测机制，每个节点维护者一个相邻节点信息列表和计数器，每个心跳包都包含着计数器的值。

还有一种广播机制，类似上学的时候，老师发了一张签名表，然后从门口座位第一个同学哪里开始传，直到全部签好名传回为止，这种方法就类似于`gossip`协议。


## KVBase何去何从？

一开始打算用`raft`算法去解决的，但是发现我要的是节点存储的时候都是一部分数据，而不是全量复制，或者是说不是`强一致的数据`，所以打算用`gossip`协议和`一致性哈希`去做分布式系统初步骨架，我目前来看我觉得`kvbase`注重`最终一致性`，类似于`Apache Cassandra`的架构那样。

## 总结

上面这些常见的问题，当然上面我还没有详细写，如果真的要深挖的话，我估计都能自己写一本书出来了，系统设计就要在设计的时候考虑到这些问题，装上熔断、退避、验证、协调、数据版本一致等等故障检测机制，这也开发者带来很大的挑战，当然我个人是这么认为的，如果有朋友对这块感兴趣可以去看看一本书《数据库系统内幕》，本文我就是对阅读这本书后的总结和笔记整理，分布式系统这如果真的想做健全可用真的是一件很有挑战的事情，因为改出错地方肯定会出错！！！


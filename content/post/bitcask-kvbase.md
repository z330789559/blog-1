---
title: "Bitcask Kvbase"
date: 2021-12-21T18:14:23+08:00
---
## 天上飞的概念，也要落地的实现

`Bitcask`相信如果你了解或者做过存储相关的工作的，相信或多或少都了解过，它是由`basho`的`bitcask`论文所设计的`key-value`存储引擎，相关的应用有`Riak`这个数据库，`Riak`是以 `Erlang` 开发的一个高度可扩展的分布式数据存储，`Riak`的实现是基于`Amazon`的`Dynamo`论文实现的，说到[Dynomo](https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf "Dynomo")那是另外一篇论文的事情了（如果你想看中文版我上一篇文章就是翻译的`Dynamo`论文可以找找历史记录），`Dynamo`他们也根据自己的实际应用做了相应的优化，言归正传，这篇我还是围绕`Bitcask`存储引擎在`KVBase`落地的实现写写。


## Bitcask怎么工作的？

`Bitcask`是使用`RAM` 存储指向值的文件指针的哈希映射，索引用于高效写入的日志结构文件系统，二者组合而成的存储引擎，可能光文章描述可能很难理解，我下面画了几幅图。

`Bitcask`主要几个组件，我个人认为最重要也就`3`个组件：

- `Index Map` 全局索引映射
- `Active File` 当前可写文件的指针
- `Compressor` 负责脏数据压缩和整理进程

![Bitcask](https://tva1.sinaimg.cn/large/008i3skNgy1gxkf3rchqzj319e0u0acp.jpg)

`key`会存储在内存中以便快速查找，所有的`value`都存储于磁盘中，这种方法特点是以追加的方式写磁盘，即写操作是有序的，这样可以减少磁磁盘的寻道时间，是一种高吞吐量的写入方案，在更新数据时，也是把新数据追加到文件的后面，然后更新一下数据的文件指针映射即可。

当写操作发生时，`keydir`被原子的更新，更新成新的文件上的位置，文件上的老数据还在磁盘上，新的读操作会使用新的`keydir`，如果在并发大量请求情况下，这个估计会出现性能瓶颈的地方之一，这个坑我会在后面说说怎么去优化。



![Keydir](https://tva1.sinaimg.cn/large/008i3skNgy1gxkf0pw880j31ei0u0n0k.jpg)

`Active File`是在磁盘上的文件，每条数据对应数据格式如下图：

![Data entry](https://tva1.sinaimg.cn/large/008i3skNgy1gxkffli4bdj31e40u0gpv.jpg)

每条数据都一条一条以行的形式存储在数据文件里，如图上通过特殊的编码方式就可以得到`key`的大小和`value`的大小，然后就可以找到`key`的位置和`value`的内容然后反编码解码操作，数据在写入的时候是被编码成二进制的存储的，补充一下`CRC`是`循环校验码`，是数据通信领域中最常用的一种差错校验码，毕竟数据库里面的数据有时候需要在网络上进行传输。

![Riak的bitcask数据目录结构文件](https://tva1.sinaimg.cn/large/008i3skNgy1gxkiflhxcnj314i0u0n12.jpg)

## Bitcask缺陷

> 这种基于内存的索引的必然有问题的，例如机器重启，内存数据丢了，索引就无法对磁盘数进行映射，还有上面提到的全局的读写必须经过这个`keydir`进行定位到数据源，另外读取数据的依赖于操作系统内核的文件系统缓存，没有用户态实现，不可控。


1. 为了解决重启内存索引数据丢失的问题，`Bitcask`论文中提到了`hint file`文件，如果是直接扫描`data`文件然后来建立索引是一件非常耗时的工作，要调用操作系统打开多个数据文件，然后来回遍历记录，官方解决方案是在一部分`data`文件记录对应一个`hint`文件，扫描一个`hint file`相比去读取`data`文件要省事的多，里面记录如下：

![hint file](https://tva1.sinaimg.cn/large/008i3skNgy1gxkjx7ksnwj31dc0u0go9.jpg)

2. 第二个问题就是数据都是增删改查的，数据被删除是肯定会发生的，那么`Bitcask`作为一个只追加记录的引擎，怎么解决？随着记录不断增多，数据文件也会变得更大，为了节省空间，`bitcask`采用`merge`的方式剔除脏数据，`merge`期间会影响到服务的访问，如果想保留原始的数据文件，可以使用谷歌的`Snappy`压缩算法，在磁盘`IO`和`cpu`之间做一个权衡，以便使程序跑得更快，`Snappy`就是这样一种快速的数据压缩算法。对于一个核的`i7`处理器（`64`位模式），能达到`250M/s`以上的处理速度。

**针对这个问题，我对`KVBase`采用的`bit`桶`key`标记删除策略，当被删除的`key`达到一定数量的时候（通过算法去判定是否需要），如果需要就会启动一个独立进程去访问为未被标记的数据，并且开始整理到新的数据文件中，在此期间服务依然可用，整理完成之后我只需要原子操作把内存映射的指针指向新的索引上，这个过程比`bitcask`论文用的整个合并数据文件要好得多，不会因为整理过程中整个服务被阻塞着，最多就是短暂消耗一点内存空间罢了，整理完成之后清理废弃内存数据和内存空间，这也是牺牲空间换取可用性和时间。我承认这个设计借鉴了`Golang`原生的`map`的扩容策略，如果你对`go`原生的`map`有了解的话，或者是看过`go`源代码的话，应该很清楚。**


## KVBase的设计

要知道`Bitcask`是以追加的方式写磁盘，即写操作是有序的，这样可以减少磁磁盘的寻道时间，是一种高吞吐量的写入设计，写性能肯定是没有得话说的。但是读取数据是通过内存中索引进行查询读取，虽然`key`的定位是通过`hash`进行`O(1)`时间复杂度，但是如果数据存放在老数据文件里面，此时就要调用系统内核`api`去读取文件，会产生短暂的`io`开销。

![KVBase的设计](https://tva1.sinaimg.cn/large/008i3skNgy1gxklm98csgj31m20u0n33.jpg)

为了解决读取性能的问题，我在`KVBase`上加了层`cache`并且这层`cache`是支持分布式的，为什么这么做？要知道数据存储问题底层的`Bitcask`引擎已经解决了，写入数据不就是为了数据不能丢吗？这个问题已经解决了，大部分写数据场景估计也不多，大部分都是读数据场景，所以加一层基于内存的`cache`为读取数据，提供保障，为了解决内存空间大小没有磁盘空间大的问题，引入一些内存淘汰算法，保证空间问题，当然具体哪个我还在考量，或者像`redis`那样从内存数据里面随机抽取一部分数据，如果超过`4/1`的数据即将过期，那么就重复抽样几次：

![淘汰策略](https://tva1.sinaimg.cn/large/008i3skNgy1gxklzr1toyj31h00nmq7n.jpg)

底层存储已经和`cache`被单独抽象出来了，以后就算换掉底层的存储引擎实现也不会影响上层`cache`，在`KVBase`我个人认为`存储引擎`只是为`cache`服务的，`cache`被抽象成了分布式的了，意味着后面我可以动态分配数据存储节点，这个就没有什么好讲的了。

## 小结
如果这篇文章对读者有什么启发，那我很高兴，文章也是一种技术分享，如果你对这块感兴趣，有什么想吐槽的，我非常欢迎交流，实质来一起`coding`。


![围观群](https://tva1.sinaimg.cn/large/008i3skNgy1gxkmqmjvvrj30kw0qqwgq.jpg)